# 2 基础

## 2.1 概率论

### 2.1.1 概率分布

### 2.1.2 概率论的基本概念

### 2.1.3 随机变量与联合分布

### 2.1.4 独立于条件独立

#### 2.1.4.1 独立

正如我们所提到的，我们常常希望$P(\alpha|\beta)$ 与$p(\alpha)$ 不相等。也就是说，观察到$\beta$ 事件发生改变了我们所知的$\alpha$ 的概率。但是，在某些情况下，相等的也是会发生的，即 $P(\alpha|\beta)=P(\alpha)$ 。也就是说，观察到$\beta$ 事件发生无法改变我们所知的$\alpha$ 的概率。

（定义2.2：独立事件）若$P(\alpha|\beta)=P(\alpha)$ 或$P(\beta)=0$ ，则我们说对于概率分布P，事件 $\alpha$ 与事件 $\beta$ 相互独立，记作$P \models (\alpha \perp \beta)$ 

我们还能为独立性提供另一个定义：

（命题2.1）一个分布P满足$\alpha \perp \beta$ 当且仅当 $P(\alpha \cap \beta) = P(\alpha) P(\beta)$

证明：略

这个定义告诉我们，独立是一个对称的记号，也就是说，$(\alpha \perp \beta)$ 暗示了 $(\alpha \perp \beta)$

（例2.3）例如，假设我们扔了两个硬币，并且让α为事件“第一次扔的结果是硬币向上”，而β为事件“第二次扔的结果是硬币向上。不难说服自己我们期望这两个事件是相互独立的。 知道β为真不会改变我们的α概率。 在这种情况下，我们看到导致事件发生的两个不同的物理过程（即抛硬币），这使我们很直观地看出两者的概率是独立的。 在某些情况下，相同的过程可能导致独立事件。 例如，考虑事件α表示“骰子结局是偶数”，而事件β表示“骰子结局是1或2”。很容易检查骰子是否公平（六个可能结果中的每一个都有概率 1），那么这两个事件是独立的。

#### 2.1.4.2 条件独立

尽管独立是有用的属性，但我们很少会遇到两个独立事件。 更为常见的情况是，在给定一个附加事件的情况下两个事件是独立的。 例如，假设我们要对学生被斯坦福大学或麻省理工学院录取的机会进行推理。 由Stanford表示事件是“斯坦福大学”（Stanford）入学，由MIT表示是“麻省理工学院（MIT）”入学。在大多数合理的发行中，这两个事件不是独立的。 如果我们得知某个学生被斯坦福大学录取，那么我们对她被麻省理工学院录取的可能性的估计现在更高，因为这表明她是一个很有前途的学生。

现在，假设两所大学的决策均仅基于学生的平均绩点（GPA），并且我们知道我们的学生的GPA为A。在这种情况下，我们可能会争辩说，得知该学生被斯坦福大学录取不会改变她被麻省理工学院录取的可能性：她的GPA已经告诉我们与她被麻省理工学院录取机会相关的信息，而了解她被斯坦福大学录取的情况也不会改变。 正式的陈述是

$P(MIT\mid Stanford,GradeA)=P(MIT\mid GradeA)$

在这种情况下，我们说在给定GradeA时MIT条件独立于Stanford。

（定义2.3：条件独立）若$P(\alpha\mid\beta\cap\gamma)=P(\alpha\mid\gamma)$ 或 $P(\beta \cap \gamma)=0$ ，则我们说对于概率分布P，在给定事件$\gamma$ 的结果后，事件 $\alpha$ 条件独立于事件 $\beta$ 

很容易扩展我们在（无条件）独立性情况下所看到的论据，以给出另一个定义。

（命题2.2）一个分布P满足$(\alpha \perp \beta \mid \gamma)$ 当且仅当 $P(\alpha \cap \beta) = P(\alpha\mid\gamma) P(\beta\mid\gamma)$

#### 2.1.4.3 随机变量的条件独立

到目前为止，我们一直专注于事件之间的独立性。 因此，我们可以说两个事件是独立的，例如一个投掷正面向上，第二个投掷也正面向上。 但是，我们想说抛硬币的任何结果都是独立的。 为了捕获此类陈述，我们可以检查独立性对随机变量集的概括。

（定义2.4：条件独立，边际独立）令X，Y，Z为随机变量集。若对于任意的$x \in Val(X)，y \in Val(Y), z \in Val(Z)$ ，分布P满足$(X = x \perp Y = y\mid Z = z)$，则我们说对于给定的服从分布P的Z，X条件独立于Y。 集合Z中的变量常被称作是可观察的。 若将Z设置为空集，则应书写成$(X \perp Y)$而不是$(X \perp Y\mid \empty)$，并称X和Y在边际上是独立的。

因此，对随机变量的独立性声明是对随机变量所有可能值的通用量化。

条件独立性的另一个定义随即出现：

（命题2.3）一个分布P满足$(X \perp Y \mid Z)$ 当且仅当 $P(X, Y) = P(X\mid Z) P(Y\mid Z)$

假如我们已经了解条件独立了，我们能否总结出分布中也必须成立的其它独立性？我们已经看过这个例子：

- 对称性：

  $(X \perp Y\mid Z) \Rightarrow (Y\perp X \mid Z) \tag{2.7}$

还有其他一些性质可以满足条件独立性，并且通常为证明分布的重要性质提供了一种非常干净的方法。 一些关键性质是：

- 分解性

  $(X\perp Y,W\mid Z)\Rightarrow(X\perp Y\mid Z) \tag{2.8}$

- Weak union:

  $(X\perp Y,W\mid Z)\Rightarrow(X\perp Y\mid Z,W) \tag{2.9}$

- Contraction:

  $(X\perp W\mid Z, Y)\&(X\perp Y \mid Z)\Rightarrow(X\perp Y, W\mid Z) \tag{2.10}$

附加的重要属性通常不成立，但确实存在于分布的重要子类中。

（定义2.5 正分布）若对所有满足$\alpha \ne \empty$ 的事件 $\alpha \in S$ 有 $P(\alpha)>0$ ，则我们说分布P是正的。 

对于正分布，我们有以下性质：

- Intersection: 对于正分布，以及对于互不相交的集合X，Y，Z，W：

  $(X\perp Y\mid Z,W)\&(X\perp W\mid Z,Y)\Rightarrow (X\perp Y,W\mid Z) \tag{2.11}$

这些性质的证明并不困难。 例如，为证明分解性，假定$(X\perp Y,W\mid Z)$成立。 然后，根据条件独立性的定义，我们得到$P(X,Y,W\mid Z)= P(X\mid Z)  P(Y,W\mid Z)$。 现在，使用概率和算术的基本规则，我们可以进行演算

$P(X,Y\mid Z)$  $=\sum\limits_wP(X,Y,w\mid Z)$

​						$=\sum\limits_wP(X\mid Z)P(Y,w\mid Z)$

​						$=P(X\mid Z)\sum\limits_wP(Y,w\mid Z)$

​						$=P(X\mid Z)P(Y\mid Z)$

我们在这里使用的唯一性质被称为"reasoning by cases"（请参阅练习2.6）。 我们得出结论$(X\perp Y\mid Z)$。

### 2.1.5 查询一个分布

### 2.1.6 连续空间

### 2.1.7 期望与方差

## 2.2 图论

# Part I - 表示

# 3 贝叶斯网络表示

我们的目标是要表示一组随机变量$X = \{X_1, ..., X_n\}$上的联合分布P。即使在最简单的情况下（这些变量都是二进制值），联合分布也需要指定$2^{n}-1$个值 —— $x_1, ..., x_n$的 $2^n$ 种不同赋值的概率。对于除最小n以外的所有值，**从每个角度来看，联合分布的显式表示都是无法管理的。从计算上来说，开销非常大，而且通常因太大而无法存储在内存中。从认知上讲，不可能从人类专家那里获得这么多数字。此外，数字非常小，与人们可以合理考虑的事件不符。从统计学上讲，如果我们想从数据中学习分布，我们将需要大量数据以便可靠地估计许多参数。这些问题一直是专家系统采用概率方法的主要障碍，直到本书所述的方法论得到发展为止**。

在本章中，我们首先说明如何使用分布中的独立性以更紧凑地表示这种高维分布。然后，我们说明组合数据结构（有向无环图）如何为我们提供一种通用建模语言，以便在我们的联合分布的表示中利用这种类型的结构。

## 3.1 **利用独立性** 

我们在本章中探讨的紧凑表示形式基于两个关键思想：分布的独立性的表示法，和参数化的合理使用，使我们能够利用这些更细粒度的独立性。

### 3.1.1 **独立随机变量** 

为了激发我们的讨论，考虑一个简单的情形，在该情形中我们知道每个$X_i$代表抛第i个硬币的结果。 在这种情况下，我们通常假设任意两次抛硬币在边际上是独立(marginally independent)的（定义2.4），因此我们的分布P对于任何i，j都将满足($X_i \perp X_j$)。 更一般地讲（严格来说，更普遍地情况请看练习3.1），我们假设变量X和Y的任何不相交的子集都满足($X \perp Y$)。 因此，我们有：

$P(X_1,...,X_n) ＝ P(X_1)P(X_2) ... P(X_n)$

如果我们使用联合分布的标准参数化，则这种独立性结构会被忽视，并且分布的表示需要 $2^n$ 个参数。 但是，我们可以使用一组更自然的参数来指定这种分布：如果$\theta_i$是硬币i正面向上的概率，则可以使用n个参数$\theta_1, ..., \theta_n$来指定联合分布P。 这些参数隐式指定联合分布中的$2^n$个概率。 例如，所有硬币正面向上的概率就是$\theta_1\cdot\theta_2\cdot...\cdot\theta_n$。 更一般地，当$x_i = x^1_i$时让$\theta_{x_i}=\theta_i$，当$x_i = x^0_i$时让$\theta_{x_i}=1-\theta_i$，我们可以定义：

$P(x_1, ..., x_n) = 􏰞\prod\limits_i\theta_{x_i} \tag{3.1}$

这种表示是有限的，通过选择$\theta_1, \theta_2, ..., \theta_n$的值，还有很多分布都是我们无法捕获的。这个事实不仅从直觉上很明显，而且某种程度上从更正式的观点出发也很明显。由所有联合分布构成的空间是$R^{2^n}$的$2^n−1$维子空间 —— 集合$\{(p_1,...,p_{2^n}) \in R^{2^n}: p_1 + ... + p^{2^n} = 1\}$。或者这么说，所有如式子(3.1)所示的由乘积表示的联合分布构成的空间是$R^{2^n}$中的n维流形。

这里的一个关键概念是独立参数(independent parameters)的概念 —— 独立参数的值不是由其他参数确定的。例如，当在一个k维空间上指定任意多项分布时，我们有k-1个独立参数：最后一个概率完全由前k-1个决定。在n个二元随机变量上有任意联合分布的情况下，独立参数的数量为$2^n −1$。另一方面，表示n次独立重复抛硬币的二项分布的独立参数数量为n。因此，分布的两个空间不能相同（尽管在这种简单情况下，这种说法似乎微不足道，但事实证明，它是比较不同表示形式表达能力的重要工具）。

如这个简单的示例所示，某些分布族（在这种情况下，是由n个独立随机变量生成的分布）允许进行另一种参数化，该参数化比朴素的表示形式，也就是显式联合分布要紧凑得多。当然，在大多数实际应用中，随机变量并不是边际独立的。但是，这种方法的一般化将成为我们解决方案的基础。

### 3.1.2 条件参数化

让我们从一个说明基本直觉的简单示例开始。 考虑一下一家公司试图聘用应届大学毕业生所面临的问题。 该公司的目标是雇用聪明的员工，但无法直接测试智力。 但是，该公司可以访问学生的SAT分数，该分数仅供参考，不能完全说明问题。 因此，我们的概率空间是由两个随机变量Intelligence (I)和SAT (S)引起的。 为了简单起见，我们假设每个变量都采用两种值：$Val(I)= \{i_1，i_0\}$，分别代表智力高($i_1$)和智力低($i_0$)； 同样，$Val(S)= \{s_1，s_0\}$，它们分别也代表分数高和分数低。因此，在这种情况下，我们的联合分布有四个条目。 例如，一种可能的联合分布P为

| I     | S     | P(I, S) |
| ----- | ----- | ------- |
| $i^0$ | $s^0$ | 0.665   |
| $i^0$ | $s^1$ | 0.035   |
| $i^1$ | $s^0$ | 0.06    |
| $i^1$ | $s^1$ | 0.24    |

（表(3.2)）

但是，存在另一种更自然的方式来表示相同的联合分布。 使用条件概率的链式规则（请参阅公式(2.5)），我们可以得出

$P(I, S) = P(I)P(S\mid I)$

直观地，我们以更符合因果关系的方式表示这个过程。首先， 存在各种因素（遗传，成长……），（随机地）决定了学生的智力。 他在SAT上的表现（随机地）取决于他的智力。 我们注意到，我们构建的模型不需要遵循因果直觉，但通常会遵循因果直觉。 稍后我们将继续探讨这个问题。

从数学角度看，该方程式导致以下表示联合分布的替代方法。 我们将以$P(I)$和$P(S\mid I)$的形式指定联合分布，而不是指定各种联合项$P(I, S)$。 因此，比方说，我们可以使用以下两个表来表示方程式(3.2)的联合分布，一个表代表I上的先验分布，另一个代表给定I的S的条件概率分布(CPD):

| $i^0$ | $i^1$ |
| ----- | ----- |
| 0.7   | 0.3   |

| I     | $s^0$ | $s^1$ |
| ----- | ----- | ----- |
| $i^0$ | 0.95  | 0.05  |
| $i^1$ | 0.2   | 0.8   |

（表(3.3)） 

CPD $P(S \mid I)$表示学生在两种可能的情况下成功通过SAT的概率：学生的智力低和智力高。 CPD断言，智力低的学生极不可能获得高SAT分数($P(s^1 \mid i^0)= 0.05$)；另一方面，智力高的学生很可能（但不确定）会获得较高的SAT分数($P(s^1|i^1)= 0.8$)。

考虑如何对这种替代表示进行参数化具有指导意义。在这里，我们使用三个二项分布，一个用于$P(I)$，两个用于$P(S | i^0)$和$P(S \mid i^1)$。因此，我们可以使用三个独立的参数$\theta_{i^1}$，$\theta_{s^1|i^1}$和$\theta_{s^1|i^0}$来参数化此表示。因为我们将联合分布表示为四结果多项分布也需要三个参数。所以，尽管条件表示比关节的显式表示更自然，但条件并不更紧凑。但是，正如我们将很快看到的，条件参数化为我们更复杂的分布的紧凑表示提供了基础。

虽然我们仅在3.2.2节中正式定义贝叶斯网络，但了解如何将此示例中的情形用一个贝叶斯网络表示是很有启发性的。如图3.1a所示，贝叶斯网络分别用两个节点表示两个随机变量I和S，从I到S的边代表此模型中依赖关系的方向。

![](./pic/001.png)

### 3.1.3 朴素贝叶斯模型

现在，我们描述最简单的示例，其中将条件参数化与条件独立性假设相结合，以产生非常紧凑的高维概率分布表示。 重要的是，与前面的完全独立随机变量示例不同，此分布中的任何变量都不会（边际）独立。

#### 3.1.3.1 学生例子

详细说明我们的示例，我们现在假设该公司在某些课程中还可以访问该学生的分数G。 在这种情况下，我们的概率空间是三个相关随机变量I，S和G的联合分布。假设I和S像以前一样，并且G取三个值$g^1，g^2，g^3$，代表等级A ，B和C，则联合分布有十二个项。

在此示例中，哪怕是还没开始考虑分布P的具体数值之前，我们可以看到独立性无济于事：对于任何合理的P，都没有独立性。 显然，学生的智力与他的SAT分数和成绩都相关。 SAT分数和成绩也不是独立的：如果我们以学生在SAT上获得高分这一事实为条件，那么他在班上获得高分的机会也有可能增加。 从而，我们可以假设，对于我们的特定分布P，$P(g^1\mid s^1)>P(g^1\mid s^0)$

但是，在这种情况下我们的分布P满足条件独立性却是很合理的。 如果我们知道学生具有很高的智力，那么SAT的高分将不再向我们提供有关该学生在课堂上的表现的信息。 更正式地：

$P(g\mid i^1, s^1)=P(g\mid i^1)$

更一般地，我们可以假设

$P \models (S \perp G \mid I) \tag{3.4}$

请注意，只有当我们假设学生的智力是与成绩和SAT分数唯一可能相关的原因时，此独立性声明才成立。 换句话说，它假设基于其他因素（例如学生参加定时考试的能力）没有相关性。 从任何形式的意义上讲，这些假设也不是“现实的”，它们通常只是我们真实信念的近似（有关更多讨论，请参见方框3.C）。

与边际独立的情况一样，条件独立允许我们提供联合分布的紧凑表示。 同样，紧凑表示法是基于非常自然的替代参数设置。 通过简单的概率推理（如公式(2.5)所示），

$P(I, S, G)=P(S, G\mid I)P(I)$

但是现在，方程式(3.4)的条件独立性假设意味着

$P(S, G\mid I)=P(S\mid I)P(G\mid I)$

因此，我们有

$P(I, S, G)=P(S\mid I)P(G\mid I)P(I) \tag{3.5}$

因此，我们将联合分布$P(I，S，G)$分解为三个条件概率分布(CPD)的乘积。 这种分解使我们立即找到所需的替代参数。 为了完全指定满足方程(3.4)的联合分布，我们需要以下三个CPD：$P(I)，P(S \mid I)和P(G \mid I)$。 前两个可能与公式(3.3)中的相同。 后者可能是

| I     | $g^1$ | $g^2$ | $g^3$ |
| ----- | ----- | ----- | ----- |
| $i^0$ | 0.2   | 0.34  | 0.46  |
| $i^1$ | 0.74  | 0.17  | 0.09  |

这三个CPD一起完全指定了联合分布（假设方程式（3.4）的条件独立）。 例如，

$P(i^1, s^1, g^2)$	$=P(i^1)P(s^1\mid i^1)P(g^2)$

​						$ = 0.3 \cdot 0.8 \cdot 0.17 = 0.0408$

再次，我们注意到，该概率模型将使用图3.1b中所示的贝叶斯网络表示。

在这种情况下，参数化比联合分布的参数化更紧凑。现在，我们有三个二项分布 —— $P(I), P(S\mid i^1)和P(S\mid i^0)$，以及两个三值多项分布 —— $P(G\mid i^1)$和$P(G\mid i^0)$。每个二项分布都需要1个独立参数，每个三值多项分布都需要2个独立参数，总共是7个独立参数。相比之下，我们的联合分布有12个条目，因此需要11个独立参数来指定这三个变量的任意联合分布。

重要的是要注意这种表示联合分布的方式的另一个优点：模块化。当我们添加新变量G时，联合分布完全改变了。如果我们使用联合分布的显式表示形式，我们将不得不写下十二个新数字。在分解式表示中，我们可以为变量I和S重用我们的局部概率模型，并仅指定G的概率模型 —— CPD $P(G\mid I)$。事实证明，此性质在对现实系统进行建模时具有不可估量的价值。

#### 3.1.3.2 一般化模型

这个例子是一个更通用的模型的实例，通常称为朴素贝叶斯(naive Bayes)模型（也被称为Idiot Bayes model ）。

![](./pic/002.png)

朴素的贝叶斯模型假设存在许多互斥的枚举类(class)，实例(instance)可被划分为其中一类。 因此，我们有一个类变量C，它采用某个集合$\{c_1, \cdots, ck\}$中的值。 在我们的示例中，类别变量是学生的智力I，并且有两类实例——智力高的学生和智力低的学生。

该模型还包括一些特征$X_1, \cdots, X_n$，这些特征的值通常已经被观察到了。 朴素的贝叶斯假设是，给定实例的类，这些特征在条件上是独立的。 换句话说，在每一类实例中，可以独立地确定不同的属性。 更正式地说，我们有

$P(C, X_1, \cdots, X_n)=P(C)\prod\limits_{i=1}^nP(X_i\mid C) \tag{3.7}$

（请参阅练习3.2）。因此，在此模型中，我们可以使用一个规模不大的因子集合来表示联合分布：先验分布P(C)，指定一个实例属于先验的不同类别的可能性，以及一组 CPD $P(X_j\mid C)$，对应了n个已知变量。 可以使用少数的参数对这些因子进行编码。 例如，如果所有变量都是二进制变量，则指定分布所需的独立参数的数量为2n +1（请参阅练习3.6）。 因此，参数的数量对变量的数量而言是线性变化的，与联合分布的显式表示的指数变化形式不同。

**方框3.A — 概念：朴素贝叶斯模型。** 尽管具有朴素的贝叶斯模型，但朴素的贝叶斯模型由于其简单性和所需参数数量少而经常在实践中使用。 该模型通常用于分类——根据给定实例的证据变量的值确定该实例最可能属于的类。 我们可能还想计算我们对该决策的信心，即我们的模型在多大程度上偏爱一个类别c^1而不是另一个类别c^2。 可以通过以下比率来解决这两个查询：

$\frac{P(C=c^1\mid x_1, \cdots, x_n)}{P(C=c^2\mid x_1, \cdots, x_n)} = \frac{P(C=c^1)}{P(C=c^1)}\prod\limits_{i=1}^n\frac{P(x_i\mid C=c^1)}{P(x_i\mid C=c^2)} \tag{3.8}$

（见练习3.2）。 该公式非常自然，因为它将c^1与c^2的后验概率计算成它们的先验概率比（式子中的第一项），乘以一组度量两个类别的发现x的相对支持的项$\frac{P(x_i\mid C=c^1)}{P(x_i\mid C=c^2)}$。

此模型用于医学诊断的早期，其中类别变量的不同值代表患者可能患有的不同疾病。 证据变量表示不同的症状，测试结果等。 请注意，该模型做出了一些通常不成立的强力假设，特别是患者最多只能患有一种疾病，并且鉴于患者的疾病，是否存在不同的症状以及不同的测试值都是 独立。 该模型可用于医学诊断，因为可解释的参数数量少，因此很容易从专家那里得出。 例如，问专家医生很自然，肺炎患者发高烧的可能性是多少。 确实，有几种早期的医学诊断系统都基于该技术，并且已经证明某些系统提供的诊断要比专家医生提供的诊断更好。

但是，后来的经验表明，该模型的强大假设降低了其诊断准确性。尤其是，该模型倾向于通过“高估”来高估某些证据的影响。例如，高血压（高血压）和肥胖症都是心脏病的有力指标。但是，由于这两种症状本身是高度相关的，因此方程式（3.8）对每个症状都包含一个相乘项，因此重复计算了它们提供的有关疾病的证据。确实，一些研究表明，朴素贝叶斯模型的诊断性能会随着特征数量的增加而降低。这种降级通常可以追溯到违反强条件独立性假设的行为。这种现象导致在此应用中使用更复杂的贝叶斯网络（具有更现实的独立性假设）（请参见方框3.D）。
尽管如此，朴素的贝叶斯模型仍然可以在各种应用中使用，特别是在从具有大量功能和相对较少实例的领域数据中学习的模型的背景下，例如使用文档中的单词作为特征；见方框17.E）。

## 3.2 贝叶斯网络

贝叶斯网络通过利用分布的条件独立性来建立与朴素贝叶斯模型相同的直觉，从而实现紧凑自然的表示。 但是，它们不限于表示满足朴素贝叶斯模型中隐含的强独立性假设的分布。 它们使我们可以灵活地将分布的表示方式调整为在当前设置下合理的独立性。

贝叶斯网络表示法的核心是有向无环图（DAG）G，其节点是我们域中的随机变量，其边缘直观地对应于一个节点对另一节点的影响。 可以通过两种非常不同的方式查看该图形G：

- 作为一种数据结构，提供了以分解的方式紧凑地表示关节分布的骨架
- 作为关于分布的一组条件独立性假设的紧凑表示

正如我们将看到的，毫无疑问，这两种观点是等效的。

### 3.2.1 学生例子回顾

我们从一个简单的玩具示例开始我们的讨论，该示例将在本书的大部分内容中以各种版本伴随我们。

#### 3.2.1.1 模型

![](./pic/003.png)

考虑我们以前的学生，但现在考虑一个稍微复杂的场景。在这种情况下，学生的成绩不仅取决于他的智力，还取决于课程的难度，学习难度由一个随机变量D表示，其域为Val（D）= {easy，hard}。我们的学生向他的教授索要推荐信。这位教授心不在and，从不记得她学生的名字。她只能看他的成绩，仅根据这些信息就可以给他写信。她的信的质量是一个随机变量L，其域为Val（L）= {strong，weak}。信件的实际质量随机取决于等级。 （取决于教授的压力和当天早上喝咖啡的质量，它可能会有所不同。）

因此，我们在这个领域中有五个随机变量：学生的智力（I），课程难度（D），年级（G），学生的SAT分数（S）和推荐信的质量（L） 。除G以外的所有变量均为二进制值，而G为三进制值。因此，联合分配有48个条目。
正如我们在图3.1的简单图示中看到的那样，贝叶斯网络是用有向图表示的，该图的节点表示随机变量，其边沿表示一个变量对另一个变量的直接影响。我们可以将图视为对自然执行的生成采样过程进行编码，其中每个变量的值都是根据自然使用仅取决于其父代的分布选择的。换句话说，每个变量都是其父母的随机函数。

基于这种直觉，本示例中最自然的分发网络结构可能如图3.3所示。 边缘反映了我们对世界运作方式的直觉。 课程难度和学生的智力是在模型中的任何变量之前独立确定的。 学生的成绩取决于这两个因素。 学生的SAT分数仅取决于他的智力。 教授推荐信的质量（假设）仅取决于班级学生的成绩。 直观地讲，模型中的每个变量仅直接取决于网络中的父变量。 我们稍后将这种直觉形式化。

贝叶斯网络表示法的第二个组成部分是一组局部概率模型，这些模型表示每个变量对其父代的依存关系的性质。一个这样的模型P（I）代表了聪明学生与不太聪明学生之间的分布。另一个P（D）表示难易类的分布。学生年级的分布是条件分布P（G | I，D）。它指定了学生年级的分布情况，这取决于学生的智力和课程难度。具体来说，对于值i，d的每个分配，我们将具有不同的分布。例如，我们可能认为，简单班上的聪明学生获得A的可能性为90％，B上学生的可能性为8％，而C上为2％。通常，只有50％的人会得到A。通常，模型中的每个变量X都与条件概率分布（CPD）关联，该条件概率分布指定了X值的分布，给定了每个值对其X的父级的联合分配。模型。对于没有父节点的节点，CPD取决于空变量集。因此，CPD变为边际分布，例如P（D）或P（I）。图3.4显示了该域CPD的一种可能选择。网络结构及其CPD是贝叶斯网络B；我们使用Bstudent引用贝叶斯网络作为我们的学生示例。

![](./pic/004.png)

我们如何使用此数据结构指定联合分布？ 考虑该空间中的某个特定状态，例如$i^1, d^0, g^2, s^1, l^0$。 从直觉上讲，可以从构成该事件的基本事件的概率中计算出该事件的概率：学生是聪明的概率； 课程容易的可能性； 聪明的学生在简单班上获得B的概率； 聪明的学生在SAT上获得高分的概率； 以及在班上获得B的学生得到弱推荐信的可能性。 此状态的总概率为：

$P(i^1, d^0, g^2, s^1, L^0)$	$ = P(i^1)P(d^0)P(g^2\mid i^1, d^0)p(s^1\mid i^1)P(l^0\mid g^2)$

​									$ = 0.3 \cdot 0.6 \cdot 0.08 \cdot 0.8 \cdot 0.4 = 0.004608$

显然，我们可以对联合概率空间中的任何状态使用相同的过程。 一般地，我们有

$P(I, D, G, S, L) = P(I)P(D)P(G\mid I, D)P(S\mid I)P(L\mid G) \tag{3.9}$

该方程式是我们贝叶斯网络链规则的第一个示例，我们将在第3.2.3.2节的一般性讨论中定义该规则。

#### 3.2.1.2 推理模式

联合分布$P_B$指定（尽管是隐式的）给定任何观测值e的任何事件y的概率$P_B(Y = y \mid E = e)$，如第2.1.3.3节所述：我们将联合分布以事件E = e为条件消除与我们的观测值e不一致的联合分布中的项，并重新归一化结果项的总和，使之为1；我们通过将后验分布中与y一致的所有项的概率相加来计算事件y的概率。为了说明这一过程，让我们考虑一下我们的$B^{student}$网络，看看随着证据的获得，各种事件的概率如何变化。

考虑一个特定的学生，乔治，我们想使用我们的模型对这个学生进行推理。我们可能会问乔治在Econ101中得到他的教授强烈推荐的可能性(l^1)。若对George或Econ101一无所知，则这个概率约为50.2％。更准确地说，令$P_{B^{student}}$为先前的BN定义的联合分布；那么我们$P_{B^{student}}(l^1)\approx 0.502$。现在我们发现乔治不是那么聪明(i^0)；他从Econ101教授那里收到一封有力信的可能性下降到38.9％左右；也就是说，$P_{B^{student}}(l^1\mid i^0)\approx 0.389$。现在，我们进一步发现Econ101是一个简单的课(d^0)。乔治从教授那里收到一封好书的概率现在为$P_{B^{student}}(l^1\mid i^0, d^0)\approx 0.513$。诸如此类的查询是我们预测各种因素（例如乔治的智力）的“下游”影响的因果推理(causal reasoning)或预测(prediction)的例子。

现在，考虑Acme Consulting的招聘人员，尝试根据我们以前的模型来决定是否雇用George。先验地，招聘人员认为，乔治有30％的机率是聪明的。他获得了特定班级Econ101的乔治成绩记录，并且看到乔治在该课程中中获得了C(g^3)。他认为乔治具有高智商的可能性大大降低，下降到7.9％左右。也就是说，$P_{B^{student}}(i^1\mid g^3)\approx 0.079$。我们注意到，该课程是困难课程的可能性也从40％上升到62.9％。

现在，假设招聘者（对George来说十分幸运）丢失了George的成绩单，并且只收到了George教授在Econ101中的弱推荐信（不足为奇）。 乔治具有高智能的可能性仍然下降，但仅下降到14％：$P_{B^{student}}(i^1 \mid l^0)\approx 0.14$。 请注意，如果招聘人员同时具有分数和推荐信，则我们拥有与只有分数的概率相同：$P_{B^{student}}(i^1\mid g^3, i^0)\approx 0.079$;；我们将重新讨论这个问题。 诸如此类的查询是我们从结果(effects)到原因(causes)的推理，是证据推理(evidential reasoning)或解释(explanation)的例子。

最后，乔治将他的SAT分数提交给了招聘者，令人惊讶的是，他的SAT分数很高。 乔治具有高智商的概率从7.9％急剧上升至57.8％：$P_{B^{student}}(i^1\mid g^3, s^1)\approx 0.578$。 直观上，高SAT成绩胜过较差成绩的原因是，智力低的学生极不可能在SAT上获得高分，而智力高的学生仍能获得C。 但是，聪明的学生更有可能在难度高的课程上获得C。 确实，我们看到Econ101是高难度课程的可能性从我们之前看到的62.9％上升到大约76％。

最后一种推理模式特别有趣。 有关SAT的信息为我们提供了有关学生智力的信息，再结合该学生的学业成绩，可以告诉我们有关该课程的难度。 实际上，我们有一个关于成绩变量的起因(causal factor)——学生智力——为我们提供了另一个起因——课程难度——的信息。

让我们单纯从形式上检查此模式。就像我们说过的，$P_{B^{student}}(i^1\mid g^3)\approx 0.079$。另一方面，如果现在我们发现Econ101是一个高难度课程，则我们的$P_{B^{student}}(i^1\mid g^3, d^1)\approx 0.11$。实际上，我们至少对Econ101中乔治的分数做了部分解释。举一个更加惊人的例子，如果乔治在Econ 101中获得B，则我们的$P_{B^{student}}(i^1\mid g^2)\approx 0.175$。另一方面，如果Econ101是一个高难度课程，我们得到$P_{B^{student}}(i^1\mid g^2, d^1)\approx 0.34$。实际上，我们已经通过课程的困难辩解(explaining away)了成绩差的原因。**辩解(explaining away)是称为原因间推理(intercausal reasoning)的一般推理模式的一个实例，其中相同结果的不同原因可以相互作用。这种推理是人类推理中非常普遍的模式。**例如，当我们发烧，嗓子疼，担心单核细胞增多症时，得知自己患有流感，我们就大为放心。显然，患有流感并不能阻止我们患有单核细胞增多症。但是，得了流感后，我们可以对症状进行另一种解释，从而大大降低了单核细胞增多症的可能性。

提供证据的另一种解释的直觉可以非常精确。如练习3.3所示，如果流感是确定性地引起症状的，则单核细胞增多症的可能性将降至其先前的可能性（观察到任何症状之前的可能性）。另一方面，如果流感可能在没有引起这些症状的情况下发生，则单核细胞增多症的可能性会下降，但仍比其基本水平高一些。但是，解释并不是因果间推理的唯一形式。影响可以向任何方向发展。例如，考虑一种情况，即有人被发现死亡并可能被谋杀。犯罪嫌疑人具有动机和机会的概率都在上升。如果我们现在发现嫌疑人有动机，那么他有机会的可能性就会上升（请参阅练习3.4）。

需要强调的是，尽管我们的解释使用了诸如原因和证据之类的直观概念，但是我们进行的概率计算并没有什么神秘之处。可以简单地通过生成联合分布（如公式(3.9)中定义）并直接从中计算各种事件的概率来复制它们。

### 3.2.2 贝叶斯网络中的基本独立性

现在，我们准备提供贝叶斯网络结构语义的正式定义。 我们希望正式定义与我们的示例中开发的直觉相匹配。

（定义3.1 贝叶斯网络结构，局部独立性）贝叶斯网络结构G是有向无环图，其节点表示随机变量$X_1, \cdots X_n$。 令$Pa^G{X_i}$表示G中$X_i$的所有父亲，$NonDescendants_{X_i}$表示图中不是$X_i$的后代的变量。 于是，G就编码了以下被称作局部独立性的记为$I_l(G)$的条件独立假设集合：

对每个变量$X_i: (X_i \perp NonDescendants_{X_i} \mid Pa^G_{X_i})$

换句话说，局部独立性指出，对每个节点$X_i$ ，在给定其父节点后，它条件独立于的非后代节点。

回到学生网络$G_{student}$，局部的马尔可夫独立性正是我们的直觉所决定的，并在公式（3.10）–公式（3.14）中指定。

![](./pic/005.png)

**专栏3.B —— 案例研究：遗传学实例。**  贝叶斯网络模型最早的用途之一（早于定义一般框架）就在遗传谱系领域。 在这种情况下，局部独立性特别直观。 在此应用程序中，我们希望对父母到孩子之间某种属性（例如血型）的传播进行建模。 一个人的血型是一个可观察到的量，取决于他的基因组成。 这种特性称为表型。 一个人的基因组成被称为基因型。

为了正确地模拟这种情况，我们需要介绍一些遗传学背景。 人类遗传物质包括22对常染色体和一对性染色体（X和Y）。 每个染色体都包含一组遗传物质，这些遗传物质（除其他外）由决定一个人特性的基因组成。 感兴趣的染色体区域称为基因座。 一个基因座可以有几个变异，称为等位基因。

为了具体起见，我们关注常染色体染色体对。 在每个常染色体对中，一个染色体是从父亲遗传的父染色体，另一个是从母亲遗传的母染色体。 对于常染色体对中的基因，一个人有该基因的两个副本，每个染色体副本一个。 因此，该基因的一个等位基因是从该人的母亲继承的，而另一个是从该人的父亲继承的。 例如，包含编码人血型的基因的区域就是一个基因座。 这个基因有3个变体或等位基因：A，B和O。因此，一个人的基因型由有序对表示，例如⟨A，B⟩。 对于该对中的每个条目有3个选择，则有9种可能的基因型。 血型表型是基因的两个拷贝的函数。 例如，如果此人有一个A等位基因和一个O等位基因，则她观察到的血型为“ A”。如果她有两个O等位基因，则观察到的血型为“ O”。

为了代表这个领域，我们每个人都有两个变量：一个代表这个人的基因型，另一个代表她的表型。 我们用名字G（p）代表人p的基因型，用名字B（p）代表她的血型。

在此示例中，独立性假设直接来自生物学。 由于血型是基因型的函数，所以一旦我们知道一个人的基因型，有关家庭其他成员的其他证据就不会提供有关血型的新信息。 同样，遗传继承的过程也意味着独立性假设。 一旦我们知道了父母双方的基因型，我们就会知道他们每个人都可以传给后代。 因此，学习有关祖先（或后裔）的新信息不会提供有关后代基因型的新信息。 这些恰好是结果网络结构中的本地独立性，如图3.B.1中的简单族谱所示。 这里的直觉很清楚； 例如，巴特（Bart）的血型与他的姨妈塞尔玛（Selma）的血型相关，但是一旦我们知道荷马（Homer）和玛格（Marge）的基因型，两者就成为独立的。

要完全定义概率模型，我们需要指定CPD。 此模型中共有三种CPD：

- 外显率模型P（B（c）| G（c））描述了给定人基因型的特定表型（如不同血型）不同变体的概率。 对于血型，此CPD是确定性功能，但在其他情况下，依赖性可能更复杂。
- 传输模型P（G（c）| G（p），G（m）），其中c是一个人，p是她的父亲和母亲。 每个父母都有可能将其两个等位基因中的一个传播给孩子。
- 基因型先验P（G（c）），当人c在谱系中没有父母时使用。 这些是种群中一般的基因型频率。

由于多种原因，我们对血型的讨论得以简化。首先，某些表型（例如迟发性疾病）不是该基因型的确定性功能。而是，具有特定基因型的个体比具有其他基因型的个体更可能患有该疾病。其次，一个人的遗传构成由许多基因定义。一些表型可能取决于多个基因。在其他情况下，我们可能会对多种表型感兴趣，这（自然地）暗示了对几种基因的依赖性。最后，正如我们现在所讨论的，不同基因的遗传模式并非彼此独立。

回想一下，每个人的常染色体都是从她的父母之一继承的。但是，每个亲本的每个常染色体也都有两个副本。在每个父母中，这两个副本重组产生染色体，并传递给孩子。因此，巴特所继承的母亲染色体是他的母亲玛格从母亲杰基和父亲克兰西继承的染色体的组合。重组过程是随机的，但是在一代人中只有少数重组事件发生在染色体内。因此，如果Bart从其母亲从其母亲Jackie继承的染色体中继承了某个基因座的等位基因，则他也很有可能从附近基因座继承Jackie的拷贝。因此，要为多基因座遗传构建合适的模型，我们必须考虑在相邻基因座对之间发生重组的可能性。

我们可以通过引入选择器变量来促进这种建模，这些选择器变量捕获沿染色体的继承模式。特别是，对于每个基因座l和每个孩子c，我们都有一个变量S（l，c，m），如果c的母亲染色体中的基因座l是从c的祖母那里继承的，则取值为1；如果该基因座是c，则取值为2。继承自c的外祖父。对于c的父系染色体，我们有一个类似的选择器变量S（l，c，p）。现在，我们可以通过关联相邻基因座l，l'的变量S（l，c，m）和S（l'，c，m）来建模由低重组频率引起的相关。

这种类型的模型已广泛用于许多应用程序。在遗传咨询和预测中，一个人采取具有已知基因座的表型，并为家谱中的某些个体获取一组观察到的表型和基因型数据，以推断谱系中另一个人（例如，计划中的孩子）的基因型和表型。遗传数据可以包括对相关疾病位点（对于某些个体）的直接测量或与疾病位点相关的附近基因座的测量。

在连锁分析中，这项任务比较艰巨：使用一定数量的谱系从谱系数据中识别疾病基因的位置，其中大部分个体表现出疾病表型。在这里，可用数据包括谱系中许多个体的表型信息，以及染色体中已知位置的基因座的基因型信息。使用遗传模型，研究人员可以根据疾病基因相对于已知基因座的位置的不同假设，评估这些观察结果的可能性。通过反复计算网络中不同假设的概率，研究人员可以查明与疾病“相关”的区域。然后，可以将该较小的区域用作对该区域中的基因进行更详细检查的起点。这个过程至关重要，因为它可以使研究人员专注于较小的区域（例如，基因组的1 / 10,000）。

正如我们将在后面的章节中看到的那样，使用稀疏贝叶斯网络描述遗传过程的能力为我们提供了使用复杂推理算法的能力，这些推理算法使我们能够推断出大的谱系和多个基因座。它还允许我们使用算法进行模型学习，以更深入地了解遗传遗传过程，例如不同区域的重组率或不同疾病的渗透率。

### 3.2.3 图与分布

贝叶斯网络图的形式语义是一组独立性断言。 另一方面，我们的学生BN是带有CPD注释的图形，该图形通过贝叶斯网络的链规则定义了联合分布。 在本节中，我们显示这两个定义实际上是等效的。 当且仅当P可表示为与图G关联的一组CPD时，分布P满足与图G关联的局部独立性。我们从形式化基本概念开始。

### 3.2.3.1 I-Maps

我们首先定义与分布P相关的一组独立性。

（定义3.2 P中的独立性）令P为X的分布。我们将I(P)定义为在P中满足 $(X\perp Y\mid Z)$ 形式的独立性断言的集合。

现在，我们可以将“ P满足与G上的局部独立性”的陈述改写为$I_l(G) \subseteq I(P)$。 在这种情况下，我们说G是P的I-map(independency map)。但是，更广泛地定义此概念很有用，因为在本书中将使用其不同的变体。

（定义3.1 I-map）令K为与一组独立性I(K)相关的任意图对象。 我们说，如果$I(K) \subseteq I$，则K是独立性集合I的I-map。

如果G对I(P)而言是一个I-map，则我们说G对P而言是一个I-map。

从包含的方向可以看出，要使G成为P的I-map，G不会在P的独立性方面误导我们：G断言的任何独立性也必须存在于P中。相反，P可能具有其他独立性，这些独立性并没有体现在G中。

让我们通过一个非常简单的示例来说明I-map的概念。

（例子3.1）考虑两个独立随机变量X和Y的联合概率空间。 在这两个节点上有三个可能的图形：$G_\empty$，它是一个不联通的的点对X Y； $G_{X\rightarrow Y}$，其包含边$X\rightarrow Y$； 和$G_{Y\rightarrow X}$，其包含边$Y\rightarrow X$。 图$G_\empty$对假设$(X\perp Y)$进行编码。 后两个没有编码独立性假设。

考虑以下两个分布：

| X     | Y     | P(X, Y) |
| ----- | ----- | ------- |
| $x^0$ | $y^0$ | 0.08    |
| $x^0$ | $y^1$ | 0.32    |
| $x^1$ | $y^0$ | 0.12    |
| $x^1$ | $y^1$ | 0.48    |

| X     | Y     | P(X, Y) |
| ----- | ----- | ------- |
| $x^0$ | $y^0$ | 0.4     |
| $x^0$ | $y^1$ | 0.3     |
| $x^1$ | $y^0$ | 0.2     |
| $x^1$ | $y^1$ | 0.1     |

在第一个分布中，X和Y在P中独立； 例如，$P(x^1)= 0.48 + 0.12 = 0.6, P(y^1)= 0.8, P(x^1, y^1)= 0.48 = 0.6 \cdot 0.8$。 因此，$(X \perp Y) \in I(P)$，我们有$G_{\empty}$是P的I-map。实际上，所有三个图都是P的I-map：$I_l(G_{X\rightarrow Y})$是空的，所以平凡的P满足了其中的所有独立性（$G_{Y\rightarrow X}$ 也是类似的）。 在第二个分布中，$(X \perp Y) \notin I(P)$，因此$G_{\empty}$不是P的I-map。另外两个图也不是P的I-map。

#### 3.2.3.2 I-Map to Factorization

BN结构G编码一组条件独立性假设；任何使得 G是I-map的分布都必须满足这些假设。 该属性是允许使用在第3.2.1节的“学生”示例中看到的紧凑分解式表示的关键。 基本原理与我们在3.1.3节中朴素贝叶斯分解中使用的原理相同。

考虑使得我们的学生BN $G_{student}$ 是I-map的任意分布P。 我们将分解联合分布，并证明它可以分解为局部概率模型，如3.2.1节所述。 考虑联合分布P(I，D，G，L，S); 从概率的链式规则（等式(2.5)），我们可以通过以下方式分解此联合分布：

$P(I, D, G, L, S)=P(I)P(D\mid I)P(G\mid I, D)P(L\mid I, D, G)P(S\mid I, D, G, L) \tag{3.15}$

此转换不依赖任何假设。它适用于任何联合分布P。但是，它也不是很有用，因为右侧分解的条件概率既非自然也不紧凑。例如，最后一个因子要求指定24个条件概率：对于值i，d，g，l的每个分配，P（s1 | i，d，g，l）。

但是，这种形式允许我们应用由国阵产生的条件独立性假设。让我们假设Gstudent是我们分布P的I-映射。特别是，根据等式（3.13），我们具有（D⊥I）∈I（P）。由此，我们可以得出结论：P（D | I）= P（D），这使我们可以简化右侧的第二个因子。同样，我们从方程（3.10）（L L I，D | G）∈I（P）。因此，P（L | I，D，G）= P（L | G），允许我们
简化第三项。以类似的方式使用公式（3.11），我们得出

$P(I, D, G, L, S)=P(I)P(D)P(G\mid I, D)P(L\mid G)P(S\mid I) \tag{3.16}$

这种分解正是我们在3.2.1节中使用的分解。
这个结果告诉我们，联合分布中的任何项都可以计算为
因素，每个变量一个。 每个因子代表变量在网络中给定其父母的条件概率。 此因式分解适用于Gstudent是I-map的任何分布P。
现在，我们更正式地陈述和证明这一基本结果。

（定义3.4 factorization）令G为变量X1，...上的BN图。 。 。 ，Xn。 我们说，如果P可以表示为乘积，则相同空间上的分布P会根据G分解。

$P(X_1, \cdots, X_n) = \prod\limits_{i=1}^nP(X_i\mid Pa_{X_i}^G)$

该方程称为贝叶斯网络的链规则。 个体因子P（Xi | PaGXi）
被称为条件概率分布（CPD）或局部概率模型。

（定义3.5 贝叶斯网络）贝叶斯网络是一对B =（G，P），其中P分解为G，其中P指定为
与G的节点相关联的一组CPD。 分布P通常标注为PB。

现在，我们可以证明对Gstudent观察到的现象更普遍。

（定理3.1）令G为一组随机变量X的BN结构，令P为同一空间上的联合分布。 如果G是P的I-map，则P根据G分解。

证明：略

因此，BN结构G隐含的条件独立性假设使我们能够将G为I-map的分布P分解为小的CPD。 注意，证明是有建设性的，在给定分布P和图G的情况下，提供了一种精确的算法来构造因式分解。

所得的因式分解表示可以实质上更紧凑，尤其是对于稀疏结构。

