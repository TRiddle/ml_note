

# 2 基础

## 2.1 概率论

### 2.1.1 概率分布

### 2.1.2 概率论的基本概念

### 2.1.3 随机变量与联合分布

### 2.1.4 独立于条件独立

#### 2.1.4.1 独立

正如我们所提到的，我们常常希望$P(\alpha|\beta)$ 与$p(\alpha)$ 不相等。也就是说，观察到$\beta$ 事件发生改变了我们所知的$\alpha$ 的概率。但是，在某些情况下，相等的也是会发生的，即 $P(\alpha|\beta)=P(\alpha)$ 。也就是说，观察到$\beta$ 事件发生无法改变我们所知的$\alpha$ 的概率。

（定义2.2：独立事件）若$P(\alpha|\beta)=P(\alpha)$ 或$P(\beta)=0$ ，则我们说对于概率分布P，事件 $\alpha$ 与事件 $\beta$ 相互独立，记作$P \models (\alpha \perp \beta)$ 

我们还能为独立性提供另一个定义：

（命题2.1）一个分布P满足$\alpha \perp \beta$ 当且仅当 $P(\alpha \cap \beta) = P(\alpha) P(\beta)$

证明：略

这个定义告诉我们，独立是一个对称的记号，也就是说，$(\alpha \perp \beta)$ 暗示了 $(\alpha \perp \beta)$

（例2.3）例如，假设我们扔了两个硬币，并且让α为事件“第一次扔的结果是硬币向上”，而β为事件“第二次扔的结果是硬币向上。不难说服自己我们期望这两个事件是相互独立的。 知道β为真不会改变我们的α概率。 在这种情况下，我们看到导致事件发生的两个不同的物理过程（即抛硬币），这使我们很直观地看出两者的概率是独立的。 在某些情况下，相同的过程可能导致独立事件。 例如，考虑事件α表示“骰子结局是偶数”，而事件β表示“骰子结局是1或2”。很容易检查骰子是否公平（六个可能结果中的每一个都有概率 1），那么这两个事件是独立的。

#### 2.1.4.2 条件独立

尽管独立是有用的属性，但我们很少会遇到两个独立事件。 更为常见的情况是，在给定一个附加事件的情况下两个事件是独立的。 例如，假设我们要对学生被斯坦福大学或麻省理工学院录取的机会进行推理。 由Stanford表示事件是“斯坦福大学”（Stanford）入学，由MIT表示是“麻省理工学院（MIT）”入学。在大多数合理的发行中，这两个事件不是独立的。 如果我们得知某个学生被斯坦福大学录取，那么我们对她被麻省理工学院录取的可能性的估计现在更高，因为这表明她是一个很有前途的学生。

现在，假设两所大学的决策均仅基于学生的平均绩点（GPA），并且我们知道我们的学生的GPA为A。在这种情况下，我们可能会争辩说，得知该学生被斯坦福大学录取不会改变她被麻省理工学院录取的可能性：她的GPA已经告诉我们与她被麻省理工学院录取机会相关的信息，而了解她被斯坦福大学录取的情况也不会改变。 正式的陈述是

$P(MIT\mid Stanford,GradeA)=P(MIT\mid GradeA)$

在这种情况下，我们说在给定GradeA时MIT条件独立于Stanford。

（定义2.3：条件独立）若$P(\alpha\mid\beta\cap\gamma)=P(\alpha\mid\gamma)$ 或 $P(\beta \cap \gamma)=0$ ，则我们说对于概率分布P，在给定事件$\gamma$ 的结果后，事件 $\alpha$ 条件独立于事件 $\beta$ 

很容易扩展我们在（无条件）独立性情况下所看到的论据，以给出另一个定义。

（命题2.2）一个分布P满足$(\alpha \perp \beta \mid \gamma)$ 当且仅当 $P(\alpha \cap \beta) = P(\alpha\mid\gamma) P(\beta\mid\gamma)$

#### 2.1.4.3 随机变量的条件独立

到目前为止，我们一直专注于事件之间的独立性。 因此，我们可以说两个事件是独立的，例如一个投掷正面向上，第二个投掷也正面向上。 但是，我们想说抛硬币的任何结果都是独立的。 为了捕获此类陈述，我们可以检查独立性对随机变量集的概括。

（定义2.4：条件独立，边际独立）令X，Y，Z为随机变量集。若对于任意的$x \in Val(X)，y \in Val(Y), z \in Val(Z)$ ，分布P满足$(X = x \perp Y = y\mid Z = z)$，则我们说对于给定的服从分布P的Z，X条件独立于Y。 集合Z中的变量常被称作是可观察的。 若将Z设置为空集，则应书写成$(X \perp Y)$而不是$(X \perp Y\mid \empty)$，并称X和Y在边际上是独立的。

因此，对随机变量的独立性声明是对随机变量所有可能值的通用量化。

条件独立性的另一个定义随即出现：

（命题2.3）一个分布P满足$(X \perp Y \mid Z)$ 当且仅当 $P(X, Y) = P(X\mid Z) P(Y\mid Z)$

假如我们已经了解条件独立了，我们能否总结出分布中也必须成立的其它独立性？我们已经看过这个例子：

- 对称性：

  $(X \perp Y\mid Z) \Rightarrow (Y\perp X \mid Z) \tag{2.7}$

还有其他一些性质可以满足条件独立性，并且通常为证明分布的重要性质提供了一种非常干净的方法。 一些关键性质是：

- 分解性

  $(X\perp Y,W\mid Z)\Rightarrow(X\perp Y\mid Z) \tag{2.8}$

- Weak union:

  $(X\perp Y,W\mid Z)\Rightarrow(X\perp Y\mid Z,W) \tag{2.9}$

- Contraction:

  $(X\perp W\mid Z, Y)\&(X\perp Y \mid Z)\Rightarrow(X\perp Y, W\mid Z) \tag{2.10}$

附加的重要属性通常不成立，但确实存在于分布的重要子类中。

（定义2.5 正分布）若对所有满足$\alpha \ne \empty$ 的事件 $\alpha \in S$ 有 $P(\alpha)>0$ ，则我们说分布P是正的。 

对于正分布，我们有以下性质：

- Intersection: 对于正分布，以及对于互不相交的集合X，Y，Z，W：

  $(X\perp Y\mid Z,W)\&(X\perp W\mid Z,Y)\Rightarrow (X\perp Y,W\mid Z) \tag{2.11}$

这些性质的证明并不困难。 例如，为证明分解性，假定$(X\perp Y,W\mid Z)$成立。 然后，根据条件独立性的定义，我们得到$P(X,Y,W\mid Z)= P(X\mid Z)  P(Y,W\mid Z)$。 现在，使用概率和算术的基本规则，我们可以进行演算

$P(X,Y\mid Z)$  $=\sum\limits_wP(X,Y,w\mid Z)$

​						$=\sum\limits_wP(X\mid Z)P(Y,w\mid Z)$

​						$=P(X\mid Z)\sum\limits_wP(Y,w\mid Z)$

​						$=P(X\mid Z)P(Y\mid Z)$

我们在这里使用的唯一性质被称为"reasoning by cases"（请参阅练习2.6）。 我们得出结论$(X\perp Y\mid Z)$。

### 2.1.5 查询一个分布

### 2.1.6 连续空间

### 2.1.7 期望与方差

## 2.2 图论

# Part I - 表示

# 3 贝叶斯网络表示

我们的目标是要表示一组随机变量$X = \{X_1, ..., X_n\}$上的联合分布P。即使在最简单的情况下（这些变量都是二进制值），联合分布也需要指定$2^{n}-1$个值 —— $x_1, ..., x_n$的 $2^n$ 种不同赋值的概率。对于除最小n以外的所有值，**从每个角度来看，联合分布的显式表示都是无法管理的。从计算上来说，开销非常大，而且通常因太大而无法存储在内存中。从认知上讲，不可能从人类专家那里获得这么多数字。此外，数字非常小，与人们可以合理考虑的事件不符。从统计学上讲，如果我们想从数据中学习分布，我们将需要大量数据以便可靠地估计许多参数。这些问题一直是专家系统采用概率方法的主要障碍，直到本书所述的方法论的得到发展为止**。

在本章中，我们首先说明如何使用分布中的独立性以更紧凑地表示这种高维分布。然后，我们说明组合数据结构（有向无环图）如何为我们提供一种通用建模语言，以便在我们的联合分布的表示中利用这种类型的结构。

## 3.1 **利用独立性** 

我们在本章中探讨的紧凑表示形式基于两个关键思想：分布的独立性的表示方法，和参数化的合理使用，使我们能够利用这些更细粒度的独立性。

### 3.1.1 **独立随机变量** 

为了激发我们的讨论，考虑一个简单的设置，在该设置中我们知道每个$X_i$代表抛硬币i的结果。 在这种情况下，我们通常假设任意两次抛硬币在边际上是独立(marginally independent)的（定义2.4），因此我们的分布P对于任何i，j都将满足($X_i \perp X_j$)。 更一般地讲（严格来说，更普遍地情况请看练习3.1），我们假设变量X和Y的任何不相交的子集都满足($X \perp Y$)。 因此，我们有：

$P(X_1,...,X_n) ＝ P(X_1)P(X_2) ... P(X_n)$

如果我们使用联合分布的标准参数化，则这种独立性结构会被遮盖，并且分布的表示需要 $2^n$ 个参数。 但是，我们可以使用一组更自然的参数来指定这种分布：如果$\theta_i$是硬币i正面向上的概率，则可以使用n个参数$\theta_1, ..., \theta_n$来指定联合分布P。 这些参数隐式指定联合分布中的$2^n$个概率。 例如，所有硬币正面向上的概率就是$\theta_1\cdot\theta_2\cdot...\cdot\theta_n$。 更一般地，当$x_i = x^1_i$时让$\theta_{x_i}=\theta_i$，当$x_i = x^0_i$时让$\theta_{x_i}=1-\theta_i$，我们可以定义：

$P(x_1, ..., x_n) = 􏰞\prod\limits_i\theta_{x_i} \tag{3.1}$

这种表示是有限的，通过选择$\theta_1, \theta_2, ..., \theta_n$的值，还有很多分布都是我们无法捕获的。这个事实不仅从直觉上很明显，而且某种程度上从更正式的观点出发也很明显。所有联合分布的空间都是$R^{2^n}$的$2^n−1$维子空间 —— 集合$\{(p_1,...,p_{2^n}) \in R^{2^n}: p_1 + ... + p^{2^n} = 1\}$。另一方面，以等式(3.1)分解表示的所有联合分布的空间是$R^{2^n}$中的n维流形。

这里的一个关键概念是独立参数(independent parameters)的概念 —— 独立参数的值不是由其他参数确定的。例如，当在一个k维空间上指定任意多项式分布时，我们有k-1个独立参数：最后一个概率完全由前k-1个决定。在n个二元随机变量上有任意联合分布的情况下，独立参数的数量为$2^n −1$。另一方面，表示n次独立重复抛掷硬币的二项分布的独立参数数量为n。因此，分布的两个空间不能相同（尽管在这种简单情况下，这种说法似乎微不足道，但事实证明，它是比较不同表示形式表达能力的重要工具）。

如这个简单的示例所示，某些分布族（在这种情况下，是由n个独立随机变量生成的分布）允许进行替代的参数化，该参数化比朴素的表示形式显式联合分布要紧凑得多。当然，在大多数实际应用中，随机变量并不是边际独立的。但是，这种方法的一般化将成为我们解决方案的基础。

### 3.1.2 条件参数化

让我们从一个说明基本直觉的简单示例开始。 考虑一下一家公司试图聘用应届大学毕业生所面临的问题。 该公司的目标是雇用聪明的员工，但无法直接测试智力。 但是，该公司可以访问学生的SAT分数，该分数仅供参考，不能完全说明问题。 因此，我们的概率空间是由两个随机变量Intelligence(I)和SAT(S)引起的。 为了简单起见，我们假设每个变狼都采用两个值：$Val(I)= \{i_1，i_0\}$，分别代表高智商(i_1)和低智商(i_0)； 同样，Val(S)= {s_1，s_0}，它们分别也代表值高分和低分。因此，在这种情况下，我们的联合分布有四个条目。 例如，一种可能的联合分布P为

| I     | S     | P(I, S) |
| ----- | ----- | ------- |
| $i^0$ | $s^0$ | 0.665   |
| $i^0$ | $s^1$ | 0.035   |
| $i^1$ | $s^0$ | 0.06    |
| $i^1$ | $s^1$ | 0.24    |

（公式(3.2)）

但是，存在另一种更自然的方式来表示相同的联合分布。 使用条件概率的链式规则（请参阅公式(2.5)），我们可以得出

P(I, S)  =  P(I) P(S|I)

直观地，我们以与因果关系更兼容的方式表示流程。 各种因素（遗传，成长……）首先（随机地）确定了学生的智力。 他在SAT上的表现（随机地）取决于他的才智。 我们注意到，我们构建的模型不需要遵循因果直觉，但通常会遵循因果直觉。 稍后我们将回到这个问题。

从数学角度看，该方程式导致以下表示联合分布的替代方法。 代替指定各种联合项P(I，S)，我们将以P(I)和P(S|I)的形式进行指定。 因此，例如，我们可以使用以下两个表来表示方程式(3.2)的联合分布，一个表代表I上的先验分布，另一个代表给定I的S的条件概率分布（CPD）:

| $i^0$ | $i^1$ |
| ----- | ----- |
| 0.7   | 0.3   |

| I     | $s^0$ | $s^1$ |
| ----- | ----- | ----- |
| $i^0$ | 0.95  | 0.05  |
| $i^1$ | 0.2   | 0.8   |

（公式(3.3)） 

CPD P(S|I)表示学生在两种可能的情况下成功通过SAT的概率：学生的智力低和智力高。 CPD断言，智力低的学生极不可能获得高SAT分数（P(s1|i0)= 0.05）；另一方面，高智商的学生很可能（但不确定）会获得较高的SAT分数（P(s1|i1)= 0.8）。

考虑如何对这种替代表示进行参数化具有指导意义。在这里，我们使用三个二项分布，一个用于P(I)，两个用于P(S | i0)和P(S | i1)。因此，我们可以使用三个独立的参数$\theta_{i^1}$，$\theta_{s^1|i^1}$和$\theta_{s^1|i^0}$来参数化此表示形式。我们将联合分布表示为四结果多项分布也需要三个参数。因此，尽管条件表示比关节的显式表示更自然，但条件并不更紧凑。但是，正如我们将很快看到的，条件参数化为我们更复杂的分布的紧凑表示提供了基础。

尽管我们仅在3.2.2节中正式定义贝叶斯网络，但了解如何将此示例表示为一个示例很有启发性。如图3.1a所示，贝叶斯网络将两个随机变量I和S中的每一个都用一个节点表示，从I到S的边代表此模型中依赖关系的方向。

![](./pic/001.png)

### 3.1.3 朴素贝叶斯模型

现在，我们描述最简单的示例，其中将条件参数化与条件独立性假设相结合，以产生非常紧凑的高维概率分布表示。 重要的是，与前面的完全独立随机变量示例不同，此分布中的任何变量都不会（边际）独立。

#### 3.1.3.1 学生例子

详细说明我们的示例，我们现在假设该公司在某些课程中还可以访问该学生的分数G。 在这种情况下，我们的概率空间是三个相关随机变量I，S和G的联合分布。假设I和S像以前一样，并且G取三个值g1，g2，g3，代表等级A ，B和C，则联合分布有十二个条目。

在此示例中，甚至没有考虑分布P的具体数值方面之前，我们可以看到独立性无济于事：对于任何合理的P，都没有独立性。 显然，学生的智力与他的SAT分数和分数都相关。 SAT分数和成绩也不是独立的：如果我们以学生在SAT上获得高分这一事实为条件，那么他在班上获得高分的机会也有可能增加。 因此，我们可以假设，对于我们的特定分布P，$P(g^1\mid s^1)>P(g^1\mid s^0)$

但是，在这种情况下我们的分布P满足条件独立性是很合理的。 如果我们知道学生具有很高的智力，那么SAT的高分将不再向我们提供有关该学生在课堂上的表现的信息。 更正式地：

$P(g\mid i^1, s^1)=P(g\mid i^1)$

更一般地，我们可以假设

$P \models (S \perp G \mid I) \tag{3.4}$

请注意，只有当我们假设学生的才智是其成绩和SAT分数可能相关的唯一原因时，此独立性声明才成立。 换句话说，它假设由于其他因素（例如学生参加定时考试的能力）而没有相关性。 从任何形式的意义上讲，这些假设也不是“真实的”，它们通常只是我们真实信念的近似。 （有关更多讨论，请参见框3.C。）

与边际独立的情况一样，条件独立允许我们提供联合分布的紧凑表示。 同样，紧凑表示法是基于非常自然的替代参数设置。 通过简单的概率推理（如公式（2.5）所示），

$P(I, S, G)=P(S, G\mid I)P(I)$

但是现在，方程式（3.4）的条件独立性假设意味着

$P(S, G\mid I)=P(S\mid I)P(G\mid I)$

因此，我们有

$P(I, S, G)=P(S\mid I)P(G\mid I)P(I) \tag{3.5}$

因此，我们将联合分布P（I，S，G）分解为三个条件概率分布（CPD）的乘积。 这种分解使我们立即找到所需的替代参数。 为了完全指定满足方程式（3.4）的联合分布，我们需要以下三个CPD：P（I），P（S | I）和P（G | I）。 前两个可能与公式（3.3）中的相同。 后者可能是

| I     | $g^1$ | $g^2$ | $g^3$ |
| ----- | ----- | ----- | ----- |
| $i^0$ | 0.2   | 0.34  | 0.46  |
| $i^1$ | 0.74  | 0.17  | 0.09  |

这三个CPD一起完全指定了联合分布（假设方程式（3.4）的条件独立）。 例如，

$P(i^1, s^1, g^2)$	$=P(i^1)P(s^1\mid i^1)P(g^2)$

​						$ = 0.3 \cdot 0.8 \cdot 0.17 = 0.0408$

再次，我们注意到，该概率模型将使用图3.1b中所示的贝叶斯网络表示。

在这种情况下，参数化比联合分布的参数化更紧凑。现在，我们有三个二项分布——P(I), P(S|i1)和P(S|i0)，以及两个三值多项分布——P(G|i1)和P(G|i0)。每个二项分布都需要一个独立参数，每个三值多项分布都需要两个独立参数，总共需要七个。相比之下，我们的联合分布有12个条目，因此需要11个独立参数来指定这三个变量的任意联合分布。

重要的是要注意这种表示联合分布的方式的另一个优点：模块化。当我们添加新变量G时，联合分布完全改变了。如果我们使用联合分布的显式表示形式，我们将不得不写下十二个新数字。在分解式表示中，我们可以为变量I和S重用我们的局部概率模型，并仅指定G的概率模型——CPD P(G|I)。事实证明，此属性在对现实系统进行建模时具有不可估量的价值。

#### 3.1.3.2 一般化模型

这个例子是一个更通用的模型的实例，通常称为朴素贝叶斯(naive Bayes)模型（也被称为Idiot Bayes model ）。

![](./pic/002.png)

朴素的贝叶斯模型假设实例属于许多互斥和穷举类之一。 因此，我们有一个类变量C，它采用某个集合$\{c_1, \cdots, ck\}$中的值。 在我们的示例中，类别变量是学生的智力I，并且有两类实例——智力高的学生和智力低的学生。

该模型还包括一些特征$X_1, \cdots, X_n$，这些特征的值通常已经被观察到了。 朴素的贝叶斯假设是，给定实例的类，这些特征在条件上是独立的。 换句话说，在每一类实例中，可以独立地确定不同的属性。 更正式地说，我们有

$P(C, X_1, \cdots, X_n)=P(C)\prod\limits_{i=1}^nP(X_i\mid C) \tag{3.7}$

（请参阅练习3.2。）因此，在此模型中，我们可以使用一小组因子来表示联合分布：先验分布P(C)，指定一个实例属于先验的不同类别的可能性，以及一组 CPD $P(X_j\mid C)$，对应了n个已知变量。 可以使用少数的参数对这些因子进行编码。 例如，如果所有变量都是二进制变量，则指定分布所需的独立参数数为2n +1（请参阅练习3.6）。 因此，参数的数量在变量的数量上是线性的，与联合分布的显式表示的指数形式相反。

**方框3.A — 概念：朴素贝叶斯模型。** 尽管具有朴素的贝叶斯模型，但朴素的贝叶斯模型由于其简单性和所需参数数量少而经常在实践中使用。 该模型通常用于分类-根据给定实例的证据变量的值确定该实例最可能属于的类。 我们可能还想计算我们对该决策的信心，即我们的模型在多大程度上偏爱一个类别c^1而不是另一个类别c^2。 可以通过以下比率来解决这两个查询：

$\frac{P(C=c^1\mid x_1, \cdots, x_n)}{P(C=c^2\mid x_1, \cdots, x_n)} = \frac{P(C=c^1)}{P(C=c^1)}\prod\limits_{i=1}^n\frac{P(x_i\mid C=c^1)}{P(x_i\mid C=c^2)} \tag{3.8}$

（见练习3.2）。 该公式非常自然，因为它将c^1与c^2的后验概率计算成它们的先验概率比（式子中的第一项），乘以一组度量两个类别的发现x的相对支持的项$\frac{P(x_i\mid C=c^1)}{P(x_i\mid C=c^2)}$。

此模型用于医学诊断的早期，其中类别变量的不同值代表患者可能患有的不同疾病。 证据变量表示不同的症状，测试结果等。 请注意，该模型做出了一些通常不成立的强力假设，特别是患者最多只能患有一种疾病，并且鉴于患者的疾病，是否存在不同的症状以及不同的测试值都是 独立。 该模型可用于医学诊断，因为可解释的参数数量少，因此很容易从专家那里得出。 例如，问专家医生很自然，肺炎患者发高烧的可能性是多少。 确实，有几种早期的医学诊断系统都基于该技术，并且已经证明某些系统提供的诊断要比专家医生提供的诊断更好。

但是，后来的经验表明，该模型的强大假设降低了其诊断准确性。尤其是，该模型倾向于通过“高估”来高估某些证据的影响。例如，高血压（高血压）和肥胖症都是心脏病的有力指标。但是，由于这两种症状本身是高度相关的，因此方程式（3.8）对每个症状都包含一个相乘项，因此重复计算了它们提供的有关疾病的证据。确实，一些研究表明，朴素贝叶斯模型的诊断性能会随着特征数量的增加而降低。这种降级通常可以追溯到违反强条件独立性假设的行为。这种现象导致在此应用中使用更复杂的贝叶斯网络（具有更现实的独立性假设）（请参见框3.D）。
尽管如此，朴素的贝叶斯模型仍然可以在各种应用中使用，特别是在从具有大量功能和相对较少实例的领域数据中学习的模型的背景下，例如使用文档中的单词作为特征；见方框17.E）。

## 3.2 贝叶斯网络

贝叶斯网络通过利用分布的条件独立性来建立与朴素贝叶斯模型相同的直觉，从而实现紧凑自然的表示。 但是，它们不限于表示满足朴素贝叶斯模型中隐含的强独立性假设的分布。 它们使我们可以灵活地将分布的表示方式调整为在当前设置下合理的独立性。

贝叶斯网络表示法的核心是有向无环图（DAG）G，其节点是我们域中的随机变量，其边缘直观地对应于一个节点对另一节点的影响。 可以通过两种非常不同的方式查看该图形G：

- 作为一种数据结构，提供了以分解的方式紧凑地表示关节分布的骨架
- 作为关于分布的一组条件独立性假设的紧凑表示

正如我们将看到的，毫无疑问，这两种观点是等效的。

### 3.2.1 学生例子回顾

我们从一个简单的玩具示例开始我们的讨论，该示例将在本书的大部分内容中以各种版本伴随我们。

#### 3.2.1.1 模型

![](./pic/003.png)

考虑我们以前的学生，但现在考虑一个稍微复杂的场景。在这种情况下，学生的成绩不仅取决于他的智力，还取决于课程的难度，学习难度由一个随机变量D表示，其域为Val（D）= {easy，hard}。我们的学生向他的教授索要推荐信。这位教授心不在and，从不记得她学生的名字。她只能看他的成绩，仅根据这些信息就可以给他写信。她的信的质量是一个随机变量L，其域为Val（L）= {strong，weak}。信件的实际质量随机取决于等级。 （取决于教授的压力和当天早上喝咖啡的质量，它可能会有所不同。）

因此，我们在这个领域中有五个随机变量：学生的智力（I），课程难度（D），年级（G），学生的SAT分数（S）和推荐信的质量（L） 。除G以外的所有变量均为二进制值，而G为三进制值。因此，联合分配有48个条目。
正如我们在图3.1的简单图示中看到的那样，贝叶斯网络是用有向图表示的，该图的节点表示随机变量，其边沿表示一个变量对另一个变量的直接影响。我们可以将图视为对自然执行的生成采样过程进行编码，其中每个变量的值都是根据自然使用仅取决于其父代的分布选择的。换句话说，每个变量都是其父母的随机函数。

基于这种直觉，本示例中最自然的分发网络结构可能如图3.3所示。 边缘反映了我们对世界运作方式的直觉。 课程难度和学生的智力是在模型中的任何变量之前独立确定的。 学生的成绩取决于这两个因素。 学生的SAT分数仅取决于他的智力。 教授推荐信的质量（假设）仅取决于班级学生的成绩。 直观地讲，模型中的每个变量仅直接取决于网络中的父变量。 我们稍后将这种直觉形式化。

贝叶斯网络表示法的第二个组成部分是一组局部概率模型，这些模型表示每个变量对其父代的依存关系的性质。一个这样的模型P（I）代表了聪明学生与不太聪明学生之间的分布。另一个P（D）表示难易类的分布。学生年级的分布是条件分布P（G | I，D）。它指定了学生年级的分布情况，这取决于学生的智力和课程难度。具体来说，对于值i，d的每个分配，我们将具有不同的分布。例如，我们可能认为，简单班上的聪明学生获得A的可能性为90％，B上学生的可能性为8％，而C上为2％。通常，只有50％的人会得到A。通常，模型中的每个变量X都与条件概率分布（CPD）关联，该条件概率分布指定了X值的分布，给定了每个值对其X的父级的联合分配。模型。对于没有父节点的节点，CPD取决于空变量集。因此，CPD变为边际分布，例如P（D）或P（I）。图3.4显示了该域CPD的一种可能选择。网络结构及其CPD是贝叶斯网络B；我们使用Bstudent引用贝叶斯网络作为我们的学生示例。

![](./pic/004.png)

我们如何使用此数据结构指定联合分布？ 考虑该空间中的某个特定状态，例如$i^1, d^0, g^2, s^1, l^0$。 从直觉上讲，可以从构成该事件的基本事件的概率中计算出该事件的概率：学生是聪明的概率； 课程容易的可能性； 聪明的学生在简单班上获得B的概率； 聪明的学生在SAT上获得高分的概率； 以及在班上获得B的学生得到弱推荐信的可能性。 此状态的总概率为：

$P(i^1, d^0, g^2, s^1, L^0)$	$ = P(i^1)P(d^0)P(g^2\mid i^1, d^0)p(s^1\mid i^1)P(l^0\mid g^2)$

​									$ = 0.3 \cdot 0.6 \cdot 0.08 \cdot 0.8 \cdot 0.4 = 0.004608$

显然，我们可以对联合概率空间中的任何状态使用相同的过程。 一般地，我们有

$P(I, D, G, S, L) = P(I)P(D)P(G\mid I, D)P(S\mid I)P(L\mid G) \tag{3.9}$

该方程式是我们贝叶斯网络链规则的第一个示例，我们将在第3.2.3.2节的一般性讨论中定义该规则。

#### 3.2.1.2 推理模式

联合分布$P_B$指定（尽管是隐式的）给定任何观测值e的任何事件y的概率$P_B(Y = y \mid E = e)$，如第2.1.3.3节所述：我们将联合分布以事件E = e为条件消除与我们的观测值e不一致的联合分布中的项，并重新归一化结果项的总和，使之为1；我们通过将后验分布中与y一致的所有项的概率相加来计算事件y的概率。为了说明这一过程，让我们考虑一下我们的$B^{student}$网络，看看随着证据的获得，各种事件的概率如何变化。

考虑一个特定的学生，乔治，我们想使用我们的模型对这个学生进行推理。我们可能会问乔治在Econ101中得到他的教授强烈推荐的可能性(l^1)。若对George或Econ101一无所知，则这个概率约为50.2％。更准确地说，令$P_{B^{student}}$为先前的BN定义的联合分布；那么我们$P_{B^{student}}(l^1)\approx 0.502$。现在我们发现乔治不是那么聪明(i^0)；他从Econ101教授那里收到一封有力信的可能性下降到38.9％左右；也就是说，$P_{B^{student}}(l^1\mid i^0)\approx 0.389$。现在，我们进一步发现Econ101是一个简单的课(d^0)。乔治从教授那里收到一封好书的概率现在为$P_{B^{student}}(l^1\mid i^0, d^0)\approx 0.513$。诸如此类的查询是我们预测各种因素（例如乔治的智力）的“下游”影响的因果推理(causal reasoning)或预测(prediction)的例子。

现在，考虑Acme Consulting的招聘人员，尝试根据我们以前的模型来决定是否雇用George。先验地，招聘人员认为，乔治有30％的机率是聪明的。他获得了特定班级Econ101的乔治成绩记录，并且看到乔治在该课程中中获得了C(g^3)。他认为乔治具有高智商的可能性大大降低，下降到7.9％左右。也就是说，$P_{B^{student}}(i^1\mid g^3)\approx 0.079$。我们注意到，该课程是困难课程的可能性也从40％上升到62.9％。

现在，假设招聘者（对George来说十分幸运）丢失了George的成绩单，并且只收到了George教授在Econ101中的弱推荐信（不足为奇）。 乔治具有高智能的可能性仍然下降，但仅下降到14％：$P_{B^{student}}(i^1 \mid l^0)\approx 0.14$。 请注意，如果招聘人员同时具有分数和推荐信，则我们拥有与只有分数的概率相同：$P_{B^{student}}(i^1\mid g^3, i^0)\approx 0.079$;；我们将重新讨论这个问题。 诸如此类的查询是我们从结果(effects)到原因(causes)的推理，是证据推理(evidential reasoning)或解释(explanation)的例子。

最后，乔治将他的SAT分数提交给了招聘者，令人惊讶的是，他的SAT分数很高。 乔治具有高智商的概率从7.9％急剧上升至57.8％：$P_{B^{student}}(i^1\mid g^3, s^1)\approx 0.578$。 直观上，高SAT成绩胜过较差成绩的原因是，智力低的学生极不可能在SAT上获得高分，而智力高的学生仍能获得C。 但是，聪明的学生更有可能在难度高的课程上获得C。 确实，我们看到Econ101是高难度课程的可能性从我们之前看到的62.9％上升到大约76％。

最后一种推理模式特别有趣。 有关SAT的信息为我们提供了有关学生智力的信息，再结合该学生的学业成绩，可以告诉我们有关该课程的难度。 实际上，我们有一个关于成绩变量的起因(causal factor)——学生智力——为我们提供了另一个起因——课程难度——的信息。

让我们单纯从形式上检查此模式。就像我们说过的，$P_{B^{student}}(i^1\mid g^3)\approx 0.079$。另一方面，如果现在我们发现Econ101是一个高难度课程，则我们的$P_{B^{student}}(i^1\mid g^3, d^1)\approx 0.11$。实际上，我们至少对Econ101中乔治的分数做了部分解释。举一个更加惊人的例子，如果乔治在Econ 101中获得B，则我们的$P_{B^{student}}(i^1\mid g^2)\approx 0.175$。另一方面，如果Econ101是一个高难度课程，我们得到$P_{B^{student}}(i^1\mid g^2, d^1)\approx 0.34$。实际上，我们已经通过课程的困难辩解(explaining away)了成绩差的原因。**辩解(explaining away)是称为原因间推理(intercausal reasoning)的一般推理模式的一个实例，其中相同结果的不同原因可以相互作用。这种推理是人类推理中非常普遍的模式。**例如，当我们发烧，嗓子疼，担心单核细胞增多症时，得知自己患有流感，我们就大为放心。显然，患有流感并不能阻止我们患有单核细胞增多症。但是，得了流感后，我们可以对症状进行另一种解释，从而大大降低了单核细胞增多症的可能性。

提供证据的另一种解释的直觉可以非常精确。如练习3.3所示，如果流感是确定性地引起症状的，则单核细胞增多症的可能性将降至其先前的可能性（观察到任何症状之前的可能性）。另一方面，如果流感可能在没有引起这些症状的情况下发生，则单核细胞增多症的可能性会下降，但仍比其基本水平高一些。但是，解释并不是因果间推理的唯一形式。影响可以向任何方向发展。例如，考虑一种情况，即有人被发现死亡并可能被谋杀。犯罪嫌疑人具有动机和机会的概率都在上升。如果我们现在发现嫌疑人有动机，那么他有机会的可能性就会上升（请参阅练习3.4）。

需要强调的是，尽管我们的解释使用了诸如原因和证据之类的直观概念，但是我们进行的概率计算并没有什么神秘之处。可以简单地通过生成联合分布（如公式(3.9)中定义）并直接从中计算各种事件的概率来复制它们。

### 3.2.2 贝叶斯网络中的基本独立性

