

# 2 基础

## 2.1 概率论

### 2.1.1 概率分布

### 2.1.2 概率论的基本概念

### 2.1.3 随机变量与联合分布

### 2.1.4 独立于条件独立

#### 2.1.4.1 独立

正如我们所提到的，我们常常希望$P(\alpha|\beta)$ 与$p(\alpha)$ 不相等。也就是说，观察到$\beta$ 事件发生改变了我们所知的$\alpha$ 的概率。但是，在某些情况下，相等的也是会发生的，即 $P(\alpha|\beta)=P(\alpha)$ 。也就是说，观察到$\beta$ 事件发生无法改变我们所知的$\alpha$ 的概率。

（定义2.2：独立事件）若$P(\alpha|\beta)=P(\alpha)$ 或$P(\beta)=0$ ，则我们说对于概率分布P，事件 $\alpha$ 与事件 $\beta$ 相互独立，记作$P \models (\alpha \perp \beta)$ 

我们还能为独立性提供另一个定义：

（命题2.1）一个分布P满足$\alpha \perp \beta$ 当且仅当 $P(\alpha \cap \beta) = P(\alpha) P(\beta)$

证明：略

这个定义告诉我们，独立是一个对称的记号，也就是说，$(\alpha \perp \beta)$ 暗示了 $(\alpha \perp \beta)$

（例2.3）例如，假设我们扔了两个硬币，并且让α为事件“第一次扔的结果是硬币向上”，而β为事件“第二次扔的结果是硬币向上。不难说服自己我们期望这两个事件是相互独立的。 知道β为真不会改变我们的α概率。 在这种情况下，我们看到导致事件发生的两个不同的物理过程（即抛硬币），这使我们很直观地看出两者的概率是独立的。 在某些情况下，相同的过程可能导致独立事件。 例如，考虑事件α表示“骰子结局是偶数”，而事件β表示“骰子结局是1或2”。很容易检查骰子是否公平（六个可能结果中的每一个都有概率 1），那么这两个事件是独立的。

#### 2.1.4.2 条件独立

尽管独立是有用的属性，但我们很少会遇到两个独立事件。 更为常见的情况是，在给定一个附加事件的情况下两个事件是独立的。 例如，假设我们要对学生被斯坦福大学或麻省理工学院录取的机会进行推理。 由Stanford表示事件是“斯坦福大学”（Stanford）入学，由MIT表示是“麻省理工学院（MIT）”入学。在大多数合理的发行中，这两个事件不是独立的。 如果我们得知某个学生被斯坦福大学录取，那么我们对她被麻省理工学院录取的可能性的估计现在更高，因为这表明她是一个很有前途的学生。

现在，假设两所大学的决策均仅基于学生的平均绩点（GPA），并且我们知道我们的学生的GPA为A。在这种情况下，我们可能会争辩说，得知该学生被斯坦福大学录取不会改变她被麻省理工学院录取的可能性：她的GPA已经告诉我们与她被麻省理工学院录取机会相关的信息，而了解她被斯坦福大学录取的情况也不会改变。 正式的陈述是

$P(MIT\mid Stanford,GradeA)=P(MIT\mid GradeA)$

在这种情况下，我们说在给定GradeA时MIT条件独立于Stanford。

（定义2.3：条件独立）若$P(\alpha\mid\beta\cap\gamma)=P(\alpha\mid\gamma)$ 或 $P(\beta \cap \gamma)=0$ ，则我们说对于概率分布P，在给定事件$\gamma$ 的结果后，事件 $\alpha$ 条件独立于事件 $\beta$ 

很容易扩展我们在（无条件）独立性情况下所看到的论据，以给出另一个定义。

（命题2.2）一个分布P满足$(\alpha \perp \beta \mid \gamma)$ 当且仅当 $P(\alpha \cap \beta) = P(\alpha\mid\gamma) P(\beta\mid\gamma)$

#### 2.1.4.3 随机变量的条件独立

到目前为止，我们一直专注于事件之间的独立性。 因此，我们可以说两个事件是独立的，例如一个投掷正面向上，第二个投掷也正面向上。 但是，我们想说抛硬币的任何结果都是独立的。 为了捕获此类陈述，我们可以检查独立性对随机变量集的概括。

（定义2.4：条件独立，边际独立）令X，Y，Z为随机变量集。若对于任意的$x \in Val(X)，y \in Val(Y), z \in Val(Z)$ ，分布P满足$(X = x \perp Y = y\mid Z = z)$，则我们说对于给定的服从分布P的Z，X条件独立于Y。 集合Z中的变量常被称作是可观察的。 若将Z设置为空集，则应书写成$(X \perp Y)$而不是$(X \perp Y\mid \empty)$，并称X和Y在边际上是独立的。

因此，对随机变量的独立性声明是对随机变量所有可能值的通用量化。

条件独立性的另一个定义随即出现：

（命题2.3）一个分布P满足$(X \perp Y \mid Z)$ 当且仅当 $P(X, Y) = P(X\mid Z) P(Y\mid Z)$

假如我们已经了解条件独立了，我们能否总结出分布中也必须成立的其它独立性？我们已经看过这个例子：

- 对称性：

  $(X \perp Y\mid Z) \Rightarrow (Y\perp X \mid Z) \tag{2.7}$

还有其他一些性质可以满足条件独立性，并且通常为证明分布的重要性质提供了一种非常干净的方法。 一些关键性质是：

- 分解性

  $(X\perp Y,W\mid Z)\Rightarrow(X\perp Y\mid Z) \tag{2.8}$

- Weak union:

  $(X\perp Y,W\mid Z)\Rightarrow(X\perp Y\mid Z,W) \tag{2.9}$

- Contraction:

  $(X\perp W\mid Z, Y)\&(X\perp Y \mid Z)\Rightarrow(X\perp Y, W\mid Z) \tag{2.10}$

附加的重要属性通常不成立，但确实存在于分布的重要子类中。

（定义2.5 正分布）若对所有满足$\alpha \ne \empty$ 的事件 $\alpha \in S$ 有 $P(\alpha)>0$ ，则我们说分布P是正的。 

对于正分布，我们有以下性质：

- Intersection: 对于正分布，以及对于互不相交的集合X，Y，Z，W：

  $(X\perp Y\mid Z,W)\&(X\perp W\mid Z,Y)\Rightarrow (X\perp Y,W\mid Z) \tag{2.11}$

这些性质的证明并不困难。 例如，为证明分解性，假定$(X\perp Y,W\mid Z)$成立。 然后，根据条件独立性的定义，我们得到$P(X,Y,W\mid Z)= P(X\mid Z)  P(Y,W\mid Z)$。 现在，使用概率和算术的基本规则，我们可以进行演算

$P(X,Y\mid Z)$  $=\sum\limits_wP(X,Y,w\mid Z)$

​						$=\sum\limits_wP(X\mid Z)P(Y,w\mid Z)$

​						$=P(X\mid Z)\sum\limits_wP(Y,w\mid Z)$

​						$=P(X\mid Z)P(Y\mid Z)$

我们在这里使用的唯一性质被称为"reasoning by cases"（请参阅练习2.6）。 我们得出结论$(X\perp Y\mid Z)$。

### 2.1.5 查询一个分布

### 2.1.6 连续空间

### 2.1.7 期望与方差

## 2.2 图论

## 2.3 参考文献

## 2.4 练习

### 练习2.6

### 练习2.7

# Part I - 表示

# 3 贝叶斯网络表示

我们的目标是要表示一组随机变量$X = \{X_1, ..., X_n\}$上的联合分布P。即使在最简单的情况下（这些变量都是二进制值），联合分布也需要指定$2^{n}-1$个值 —— $x_1, ..., x_n$的 $2^n$ 种不同赋值的概率。对于除最小n以外的所有值，**从每个角度来看，联合分布的显式表示都是无法管理的。从计算上来说，开销非常大，而且通常因太大而无法存储在内存中。从认知上讲，不可能从人类专家那里获得这么多数字。此外，数字非常小，与人们可以合理考虑的事件不符。从统计学上讲，如果我们想从数据中学习分布，我们将需要大量数据以便可靠地估计许多参数。这些问题一直是专家系统采用概率方法的主要障碍，直到本书所述的方法论的得到发展为止**。

在本章中，我们首先说明如何使用分布中的独立性以更紧凑地表示这种高维分布。然后，我们说明组合数据结构（有向无环图）如何为我们提供一种通用建模语言，以便在我们的联合分布的表示中利用这种类型的结构。

## 3.1 **利用独立性** 

我们在本章中探讨的紧凑表示形式基于两个关键思想：分布的独立性的表示方法，和参数化的合理使用，使我们能够利用这些更细粒度的独立性。

### 3.1.1 **独立随机变量** 

为了激发我们的讨论，考虑一个简单的设置，在该设置中我们知道每个$X_i$代表抛硬币i的结果。 在这种情况下，我们通常假设任意两次抛硬币在边际上是独立(marginally independent)的（定义2.4），因此我们的分布P对于任何i，j都将满足($X_i \perp X_j$)。 更一般地讲（严格来说，更普遍地情况请看练习3.1），我们假设变量X和Y的任何不相交的子集都满足($X \perp Y$)。 因此，我们有：

$P(X_1,...,X_n) ＝ P(X_1)P(X_2) ... P(X_n)$

如果我们使用联合分布的标准参数化，则这种独立性结构会被遮盖，并且分布的表示需要 $2^n$ 个参数。 但是，我们可以使用一组更自然的参数来指定这种分布：如果$\theta_i$是硬币i正面向上的概率，则可以使用n个参数$\theta_1, ..., \theta_n$来指定联合分布P。 这些参数隐式指定联合分布中的$2^n$个概率。 例如，所有硬币正面向上的概率就是$\theta_1\cdot\theta_2\cdot...\cdot\theta_n$。 更一般地，当$x_i = x^1_i$时让$\theta_{x_i}=\theta_i$，当$x_i = x^0_i$时让$\theta_{x_i}=1-\theta_i$，我们可以定义：

$P(x_1, ..., x_n) = 􏰞\prod\limits_i\theta_{x_i} \tag{3.1}$

这种表示是有限的，通过选择$\theta_1, \theta_2, ..., \theta_n$的值，还有很多分布都是我们无法捕获的。这个事实不仅从直觉上很明显，而且某种程度上从更正式的观点出发也很明显。所有联合分布的空间都是$R^{2^n}$的$2^n−1$维子空间 —— 集合$\{(p_1,...,p_{2^n}) \in R^{2^n}: p_1 + ... + p^{2^n} = 1\}$。另一方面，以等式(3.1)分解表示的所有联合分布的空间是$R^{2^n}$中的n维流形。

这里的一个关键概念是独立参数(independent parameters)的概念 —— 独立参数的值不是由其他参数确定的。例如，当在一个k维空间上指定任意多项式分布时，我们有k-1个独立参数：最后一个概率完全由前k-1个决定。在n个二元随机变量上有任意联合分布的情况下，独立参数的数量为$2^n −1$。另一方面，表示n次独立重复抛掷硬币的二项分布的独立参数数量为n。因此，分布的两个空间不能相同（尽管在这种简单情况下，这种说法似乎微不足道，但事实证明，它是比较不同表示形式表达能力的重要工具）。

如这个简单的示例所示，某些分布族（在这种情况下，是由n个独立随机变量生成的分布）允许进行替代的参数化，该参数化比朴素的表示形式显式联合分布要紧凑得多。当然，在大多数实际应用中，随机变量并不是边际独立的。但是，这种方法的一般化将成为我们解决方案的基础。

### 3.1.2 条件参数化

让我们从一个说明基本直觉的简单示例开始。 考虑一下一家公司试图聘用应届大学毕业生所面临的问题。 该公司的目标是雇用聪明的员工，但无法直接测试智力。 但是，该公司可以访问学生的SAT分数，该分数仅供参考，不能完全说明问题。 因此，我们的概率空间是由两个随机变量Intelligence(I)和SAT(S)引起的。 为了简单起见，我们假设每个变狼都采用两个值：$Val(I)= \{i_1，i_0\}$，分别代表高智商(i_1)和低智商(i_0)； 同样，Val(S)= {s_1，s_0}，它们分别也代表值高分和低分。因此，在这种情况下，我们的联合分布有四个条目。 例如，一种可能的联合分布P为

| I     | S     | P(I, S) |
| ----- | ----- | ------- |
| $i^0$ | $s^0$ | 0.665   |
| $i^0$ | $s^1$ | 0.035   |
| $i^1$ | $s^0$ | 0.06    |
| $i^1$ | $s^1$ | 0.24    |

（公式(3.2)）

但是，存在另一种更自然的方式来表示相同的联合分布。 使用条件概率的链式规则（请参阅公式(2.5)），我们可以得出

P(I, S)  =  P(I) P(S|I)

直观地，我们以与因果关系更兼容的方式表示流程。 各种因素（遗传，成长……）首先（随机地）确定了学生的智力。 他在SAT上的表现（随机地）取决于他的才智。 我们注意到，我们构建的模型不需要遵循因果直觉，但通常会遵循因果直觉。 稍后我们将回到这个问题。

从数学角度看，该方程式导致以下表示联合分布的替代方法。 代替指定各种联合项P(I，S)，我们将以P(I)和P(S|I)的形式进行指定。 因此，例如，我们可以使用以下两个表来表示方程式(3.2)的联合分布，一个表代表I上的先验分布，另一个代表给定I的S的条件概率分布（CPD）:

| $i^0$ | $i^1$ |
| ----- | ----- |
| 0.7   | 0.3   |

| I     | $s^0$ | $s^1$ |
| ----- | ----- | ----- |
| $i^0$ | 0.95  | 0.05  |
| $i^1$ | 0.2   | 0.8   |

（公式(3.3)） 

CPD P(S|I)表示学生在两种可能的情况下成功通过SAT的概率：学生的智力低和智力高。 CPD断言，智力低的学生极不可能获得高SAT分数（P(s1|i0)= 0.05）；另一方面，高智商的学生很可能（但不确定）会获得较高的SAT分数（P(s1|i1)= 0.8）。

考虑如何对这种替代表示进行参数化具有指导意义。在这里，我们使用三个二项分布，一个用于P(I)，两个用于P(S | i0)和P(S | i1)。因此，我们可以使用三个独立的参数$\theta_{i^1}$，$\theta_{s^1|i^1}$和$\theta_{s^1|i^0}$来参数化此表示形式。我们将联合分布表示为四结果多项分布也需要三个参数。因此，尽管条件表示比关节的显式表示更自然，但条件并不更紧凑。但是，正如我们将很快看到的，条件参数化为我们更复杂的分布的紧凑表示提供了基础。

尽管我们仅在3.2.2节中正式定义贝叶斯网络，但了解如何将此示例表示为一个示例很有启发性。如图3.1a所示，贝叶斯网络将两个随机变量I和S中的每一个都用一个节点表示，从I到S的边代表此模型中依赖关系的方向。

![](./pic/001.png)

### 3.1.3 朴素贝叶斯模型

现在，我们描述最简单的示例，其中将条件参数化与条件独立性假设相结合，以产生非常紧凑的高维概率分布表示。 重要的是，与前面的完全独立随机变量示例不同，此分布中的任何变量都不会（边际）独立。

#### 3.1.3.1 The Student Example

详细说明我们的示例，我们现在假设该公司在某些课程中还可以访问该学生的分数G。 在这种情况下，我们的概率空间是三个相关随机变量I，S和G的联合分布。假设I和S像以前一样，并且G取三个值g1，g2，g3，代表等级A ，B和C，则联合分布有十二个条目。

在此示例中，甚至没有考虑分布P的具体数值方面之前，我们可以看到独立性无济于事：对于任何合理的P，都没有独立性。 显然，学生的智力与他的SAT分数和分数都相关。 SAT分数和成绩也不是独立的：如果我们以学生在SAT上获得高分这一事实为条件，那么他在班上获得高分的机会也有可能增加。 因此，我们可以假设，对于我们的特定分布P，$P(g^1\mid s^1)>P(g^1\mid s^0)$

但是，在这种情况下我们的分布P满足条件独立性是很合理的。 如果我们知道学生具有很高的智力，那么SAT的高分将不再向我们提供有关该学生在课堂上的表现的信息。 更正式地：

$P(g\mid i^1, s^1)=P(g\mid i^1)$

更一般地，我们可以假设

$P \models (S \perp G \mid I) \tag{3.4}$





