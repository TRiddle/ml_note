# 2 基础

## 2.1 概率论

### 2.1.1 概率分布

### 2.1.2 概率论的基本概念

### 2.1.3 随机变量与联合分布

#### 2.1.3.4 条件概率

条件概率的概念扩展到随机变量上的诱导分布。 例如，假设学生的成绩为A，我们使用符号$P(Intelligence\mid Grade = A)$来表示Intelligence所描述的事件的条件分布。请注意，在观察到该变量的情况下，随机变量的条件分布 另一个值与边际分布不同。 在我们的示例中，P（智力=高）= 0.3，而P（智力=高|等级= A）= 0.18 / 0.25 = 0.72。 因此，显然P（智能|等级= A）与边际分布P（智能）不同。 后者的分布表示我们在学习某个特定学生的其他知识之前对学生的先验知识，而条件分布则表示我们在学习了她的成绩后更加知情的分布。

我们将经常使用符号$P(X \mid Y)$表示一组条件概率分布。 直观地，对于每个Y值，此对象使用条件概率在X值上分配概率。 这种表示法使我们可以写出链式法则的简单形式：$P(X, Y) = P(X) P(Y \mid X)$，它可以扩展到多个变量的情况

$P(X_1, \cdots, X_k) = P(X_1)P(X_2\mid X_1)\cdots P(X_k\mid X_1, \cdots, X_{k-1}) \tag{2.5}$

同样，我们可以根据条件概率分布来陈述贝叶斯规则：

$P(X\mid Y) = \frac{P(X)P(Y\mid X)}{P(Y)} \tag{2.6}$

### 2.1.4 独立与条件独立

#### 2.1.4.1 独立

正如我们所提到的，我们常常希望$P(\alpha|\beta)$ 与$p(\alpha)$ 不相等。也就是说，观察到$\beta$ 事件发生改变了我们所知的$\alpha$ 的概率。但是，在某些情况下，相等的也是会发生的，即 $P(\alpha|\beta)=P(\alpha)$ 。也就是说，观察到$\beta$ 事件发生无法改变我们所知的$\alpha$ 的概率。

（**定义2.2**：独立事件）若$P(\alpha|\beta)=P(\alpha)$ 或$P(\beta)=0$ ，则我们说对于概率分布P，事件 $\alpha$ 与事件 $\beta$ 相互独立，记作$P \models (\alpha \perp \beta)$ 

我们还能为独立性提供另一个定义：

（**命题2.1**）一个分布P满足$\alpha \perp \beta$ 当且仅当 $P(\alpha \cap \beta) = P(\alpha) P(\beta)$

证明：略

这个定义告诉我们，独立是一个对称的记号，也就是说，$(\alpha \perp \beta)$ 暗示了 $(\alpha \perp \beta)$

（例2.3）例如，假设我们扔了两个硬币，并且让α为事件“第一次扔的结果是硬币向上”，而β为事件“第二次扔的结果是硬币向上。不难说服自己我们期望这两个事件是相互独立的。 知道β为真不会改变我们的α概率。 在这种情况下，我们看到导致事件发生的两个不同的物理过程（即抛硬币），这使我们很直观地看出两者的概率是独立的。 在某些情况下，相同的过程可能导致独立事件。 例如，考虑事件α表示“骰子结局是偶数”，而事件β表示“骰子结局是1或2”。很容易检查骰子是否公平（六个可能结果中的每一个都有概率 1），那么这两个事件是独立的。

#### 2.1.4.2 条件独立

尽管独立是有用的属性，但我们很少会遇到两个独立事件。 更为常见的情况是，在给定一个附加事件的情况下两个事件是独立的。 例如，假设我们要对学生被斯坦福大学或麻省理工学院录取的机会进行推理。 由Stanford表示事件是“斯坦福大学”（Stanford）入学，由MIT表示是“麻省理工学院（MIT）”入学。在大多数合理的发行中，这两个事件不是独立的。 如果我们得知某个学生被斯坦福大学录取，那么我们对她被麻省理工学院录取的可能性的估计现在更高，因为这表明她是一个很有前途的学生。

现在，假设两所大学的决策均仅基于学生的平均绩点（GPA），并且我们知道我们的学生的GPA为A。在这种情况下，我们可能会争辩说，得知该学生被斯坦福大学录取不会改变她被麻省理工学院录取的可能性：她的GPA已经告诉我们与她被麻省理工学院录取机会相关的信息，而了解她被斯坦福大学录取的情况也不会改变。 正式的陈述是

$P(MIT\mid Stanford,GradeA)=P(MIT\mid GradeA)$

在这种情况下，我们说在给定GradeA时MIT条件独立于Stanford。

**（定义2.3**：条件独立）若$P(\alpha\mid\beta\cap\gamma)=P(\alpha\mid\gamma)$ 或 $P(\beta \cap \gamma)=0$ ，则我们说对于概率分布P，在给定事件$\gamma$ 的结果后，事件 $\alpha$ 条件独立于事件 $\beta$ 

很容易扩展我们在（无条件）独立性情况下所看到的论据，以给出另一个定义。

（**命题2.2**）一个分布P满足$(\alpha \perp \beta \mid \gamma)$ 当且仅当 $P(\alpha \cap \beta) = P(\alpha\mid\gamma) P(\beta\mid\gamma)$

#### 2.1.4.3 随机变量的条件独立

到目前为止，我们一直专注于事件之间的独立性。 因此，我们可以说两个事件是独立的，例如一个投掷正面向上，第二个投掷也正面向上。 但是，我们想说抛硬币的任何结果都是独立的。 为了捕获此类陈述，我们可以检查独立性对随机变量集的概括。

**（定义2.4**：条件独立，边际独立）令X，Y，Z为随机变量集。若对于任意的$x \in Val(X)，y \in Val(Y), z \in Val(Z)$ ，分布P满足$(X = x \perp Y = y\mid Z = z)$，则我们说对于给定的服从分布P的Z，X条件独立于Y。 集合Z中的变量常被称作是可观察的。 若将Z设置为空集，则应书写成$(X \perp Y)$而不是$(X \perp Y\mid \empty)$，并称X和Y在边际上是独立的。

因此，对随机变量的独立性声明是对随机变量所有可能值的通用量化。

条件独立性的另一个定义随即出现：

（**命题2.3**）一个分布P满足$(X \perp Y \mid Z)$ 当且仅当 $P(X, Y) = P(X\mid Z) P(Y\mid Z)$

假如我们已经了解条件独立了，我们能否总结出分布中也必须成立的其它独立性？我们已经看过这个例子：

- 对称性：

  $(X \perp Y\mid Z) \Rightarrow (Y\perp X \mid Z) \tag{2.7}$

还有其他一些性质可以满足条件独立性，并且通常为证明分布的重要性质提供了一种非常干净的方法。 一些关键性质是：

- 分解性

  $(X\perp Y,W\mid Z)\Rightarrow(X\perp Y\mid Z) \tag{2.8}$

- Weak union:

  $(X\perp Y,W\mid Z)\Rightarrow(X\perp Y\mid Z,W) \tag{2.9}$

- Contraction:

  $(X\perp W\mid Z, Y)\&(X\perp Y \mid Z)\Rightarrow(X\perp Y, W\mid Z) \tag{2.10}$

附加的重要属性通常不成立，但确实存在于分布的重要子类中。

（**定义2.5** 正分布）若对所有满足$\alpha \ne \empty$ 的事件 $\alpha \in S$ 有 $P(\alpha)>0$ ，则我们说分布P是正的。 

对于正分布，我们有以下性质：

- Intersection: 对于正分布，以及对于互不相交的集合X，Y，Z，W：

  $(X\perp Y\mid Z,W)\&(X\perp W\mid Z,Y)\Rightarrow (X\perp Y,W\mid Z) \tag{2.11}$

这些性质的证明并不困难。 例如，为证明分解性，假定$(X\perp Y,W\mid Z)$成立。 然后，根据条件独立性的定义，我们得到$P(X,Y,W\mid Z)= P(X\mid Z)  P(Y,W\mid Z)$。 现在，使用概率和算术的基本规则，我们可以进行演算

$P(X,Y\mid Z)$  $=\sum\limits_wP(X,Y,w\mid Z)$

​						$=\sum\limits_wP(X\mid Z)P(Y,w\mid Z)$

​						$=P(X\mid Z)\sum\limits_wP(Y,w\mid Z)$

​						$=P(X\mid Z)P(Y\mid Z)$

我们在这里使用的唯一性质被称为"reasoning by cases"（请参阅练习2.6）。 我们得出结论$(X\perp Y\mid Z)$。

### 2.1.5 查询一个分布

### 2.1.6 连续空间

### 2.1.7 期望与方差

## 2.2 图论

### 2.2.1 点和边

图(graph)是由一组节点(node)和一组边(edge)组成的数据结构K。在本书的大部分内容中，我们都假定节点集为$X = \{X_1, \dots, X_n\}$。一对节点$X_i, X_j$可以通过有向边$Xi\rightarrow X_j$或无向边$X_i - X_j$连接。因此，边集 $E$ 是节点对的集合，其中每个节点对为$X_i\rightarrow X_j，X_j\rightarrow X_i$ ，或 $X_i-X_j$。在整本书中，我们假设，对于每对节点$X_i，X_j$，最多存在一种边类型；因此，我们既不能同时拥有$X_i\rightarrow X_j$和$Xj→Xi$，也不能同时拥有$X_i\rightarrow X_j$和$X_i - X_j$。记号$X_i\leftarrow X_j$ 等效于 $X_j\rightarrow X_i$，而记号$X_j - X_i$等效于$X_i - X_j$ 。我们使用$X_i\rightleftharpoons 􏰱X_j$表示$X_i$和$X_j$通过某个边（无论是有向（沿任何方向）还是无向）连接的情况。

在许多情况下，我们希望将注意力集中在仅包含一种或另一种边的图上。我们说如果图上的所有边都是$X_i\rightarrow X_j$ 或 $X_j\rightarrow X_i$则图是有向的(directed)。我们通常将有向图表示为G。我们说如果所有边均为$X_i - X_j$，则图是无向的(undirected)。我们将无向图表示为H。有时，我们通过忽略边的方向将普通图转换为无向图。

（**定义2.11** 图的无向表示）给定一个图$K = (X, E)$ ，它的无向表示(undirected version)是一个图$H = (X, E')$ ，其中$E' = \{X-Y:X\rightleftharpoons Y \in E\}$ 。

当我们有$X_i\rightarrow X_j\in E$ 时 ，我们就说$X_j$ 是$X_i$ 在K中的的子节点(child)，当我们有$X_i- X_j\in E$ 时 ，我们就说$X_j$ 是$X_i$ 在K中的的邻居节点(neighbors)，反之亦然。当$X \rightleftharpoons Y\in E$ 时我们说X和Y时邻接(adjcent)的。我们用$Pa_X$ 来表示X的父节点，$Ch_X$ 表示它的子节点，$Nb_X$  表示它的邻居节点。我们将X的界(boundary)定义为$Pa_X \cup Nb_X$ ，用$Boundary_X$ 来表示；对于DAG而言，这个集合只是X的父节点，对无向图而言，这个集合是X的邻居节点。图2.3是图K的一个例子。其中，我们可以看出A是C唯一的父节点，F, I 是C的子节点。C唯一的邻居借点是D，但它的邻接节点是A, D, F, I。点X的度数(degree)是与点相连的边的个数。X的入度(indegree)是有向边$Y \rightarrow X$ 的个数。图的度数指的是图中点的度数的最大值。

![](./pic/006.png)

### 2.2.2 子图

在许多情况下，我们只想考虑图中与节点的特定子集相关联的部分。

（**定义2.12** 导出子图）令$K = (\mathcal{X}, E)$，且$\mathbf{X} \sub \mathcal{X}$ ，我们将图($\mathbf{X}, E'$)定义为导出子图(induced subgraph) $K[\mathbf{X}]$ 。其中E'是所有满足$X, Y \in \mathbf{X}$ 的边$X \rightleftharpoons Y \in E$ 。

例如，图2.4a展示的就是导出子图K[C, D, I]。

我们常常会关注一种特定的将所有可能的边都包含的子图。

（**定义2.13** 完全子图）当在 $\mathbf{X}$ 上的子图中，任意两个节点都有边相连，则该子图是完全的(complete)。其中点集 $\mathbf{X}$ 常常被称作团(clique)；当不存在任何节点超集$\mathbf{Y} \supset \mathbf{X}$ ，使得Y不是一个团时，我们说团$\mathbf{X}$时最大的。

尽管节点$\mathbf{X}$ 的子集可以是任意的，但我们通常对保留图结构的某些方面的节点集感兴趣。

（**定义2.14** 上闭包）$\mathbf{X}$ 是图中节点集合的子集，若对于任意的$X \in \mathbf{X}$ 我们有$Boundary_X \sub \mathbf{X}$ ，我们就说$\mathbf{X}$ 在图K中是上闭合(upwardly closed)的。我们将包含$\mathbf{X}$的最小上闭合子点集$\mathbf{Y}$定义为$\mathbf{X}$ 的上闭包(upward closure)。我们将上述$\mathbf{Y}$上的导出子图$K[\mathbf{Y}]$定义为$\mathbf{X}$ 的上闭合子图(upwardly closed subgraph)，记为$K^+[\mathbf{X}]$ 。

例如，集合A，B，C，D，E是K中集合{C}的上闭包。{C}的上闭合子图如图2.4b所示。 {C，D，I}的上闭合子图如图2.4c所示。

### 2.2.3 路径与轨迹

使用边的基本概念，我们可以在图中定义不同类型的长连接(longer-range connections)。

（定义2.15 路径）若对任意$i = 1, \dots, k - 1$ 我们都有$X_i \rightarrow X_{i+1}$ 或$X_i - X_{i+1}$ ，则我们说$X_1, \dots X_k$ 构成图K = (X, E)中的路径。当至少有一个i使得$X_i \rightarrow X_{i+1}$ 存在时，这个路径是有向的。

（定义2.16 轨迹）若对任意$i = 1, \dots, k - 1$ 我们都有$X_i \rightleftharpoons X_{i+1}$ ，则我们说$X_1, \dots X_k$ 构成图K = (X, E)中的轨迹。

（定义2.17 连通图）若对图上任意$X_i, X_j$ 都存在一个介于它们之间的轨迹，则我们说这个图是联通的。

现在我们能够定义图中的长关系(longer-range relationships)了。

（定义2.18 祖先，后代）若图K = (X, E)中存在一个有向路径$X_1, \dots, X_k$ ，且$X_1 = X, X_k = Y$，我们就说X是Y的祖先，Y是X的后代。我们使用$Descendants_X$ 来表示X的后代，$Ancestors_X$ 来表示X的祖先，$NonDescendants_X$ 来表示集合$X - Decendants_X$中的点。

再我们的例图K中，我们有F, G, I是C的后代，C的祖先是A（通过路径A, C）和B（通过路径B, E, D, C）。

最后一个有用的概念是有向图中与其边的方向性一致的节点的排序。

（定义2.19 拓扑序）令G = (X, E)为一个图，若对$X_1, \dots, X_n$ 中的任意$X_i \rightarrow X_j \in E$ 都有i < j，则我们说$X_1, \dots, X_n$构成了相对于K的拓扑序。

Appendix A.3.1给出了一个求拓扑序的算法。

### 2.2.4 圈和环

# Part I - 表示

# 3 贝叶斯网络表示

我们的目标是要表示一组随机变量$X = \{X_1, ..., X_n\}$上的联合分布P。即使在最简单的情况下（这些变量都是二进制值），联合分布也需要指定$2^{n}-1$个值 —— $x_1, ..., x_n$的 $2^n$ 种不同赋值的概率。对于除最小n以外的所有值，**从每个角度来看，联合分布的显式表示都是无法管理的。从计算上来说，开销非常大，而且通常因太大而无法存储在内存中。从认知上讲，不可能从人类专家那里获得这么多数字。此外，数字非常小，与人们可以合理考虑的事件不符。从统计学上讲，如果我们想从数据中学习分布，我们将需要大量数据以便可靠地估计许多参数。这些问题一直是专家系统采用概率方法的主要障碍，直到本书所述的方法论得到发展为止**。

在本章中，我们首先说明如何使用分布中的独立性以更紧凑地表示这种高维分布。然后，我们说明组合数据结构（有向无环图）如何为我们提供一种通用建模语言，以便在我们的联合分布的表示中利用这种类型的结构。

## 3.1 **利用独立性** 

我们在本章中探讨的紧凑表示形式基于两个关键思想：分布的独立性的表示法，和参数化的合理使用，使我们能够利用这些更细粒度的独立性。

### 3.1.1 **独立随机变量** 

为了激发我们的讨论，考虑一个简单的情形，在该情形中我们知道每个$X_i$代表抛第i个硬币的结果。 在这种情况下，我们通常假设任意两次抛硬币在边际上是独立(marginally independent)的（定义2.4），因此我们的分布P对于任何i，j都将满足($X_i \perp X_j$)。 更一般地讲（严格来说，更普遍地情况请看练习3.1），我们假设变量X和Y的任何不相交的子集都满足($X \perp Y$)。 因此，我们有：

$P(X_1,...,X_n) ＝ P(X_1)P(X_2) ... P(X_n)$

如果我们使用联合分布的标准参数化，则这种独立性结构会被忽视，并且分布的表示需要 $2^n$ 个参数。 但是，我们可以使用一组更自然的参数来指定这种分布：如果$\theta_i$是硬币i正面向上的概率，则可以使用n个参数$\theta_1, ..., \theta_n$来指定联合分布P。 这些参数隐式指定联合分布中的$2^n$个概率。 例如，所有硬币正面向上的概率就是$\theta_1\cdot\theta_2\cdot...\cdot\theta_n$。 更一般地，当$x_i = x^1_i$时让$\theta_{x_i}=\theta_i$，当$x_i = x^0_i$时让$\theta_{x_i}=1-\theta_i$，我们可以定义：

$P(x_1, ..., x_n) = 􏰞\prod\limits_i\theta_{x_i} \tag{3.1}$

这种表示是有限的，通过选择$\theta_1, \theta_2, ..., \theta_n$的值，还有很多分布都是我们无法捕获的。这个事实不仅从直觉上很明显，而且某种程度上从更正式的观点出发也很明显。由所有联合分布构成的空间是$R^{2^n}$的$2^n−1$维子空间 —— 集合$\{(p_1,...,p_{2^n}) \in R^{2^n}: p_1 + ... + p^{2^n} = 1\}$。或者这么说，所有如式子(3.1)所示的由乘积表示的联合分布构成的空间是$R^{2^n}$中的n维流形。

这里的一个关键概念是独立参数(independent parameters)的概念 —— 独立参数的值不是由其他参数确定的。例如，当在一个k维空间上指定任意多项分布时，我们有k-1个独立参数：最后一个概率完全由前k-1个决定。在n个二元随机变量上有任意联合分布的情况下，独立参数的数量为$2^n −1$。另一方面，表示n次独立重复抛硬币的二项分布的独立参数数量为n。因此，分布的两个空间不能相同（尽管在这种简单情况下，这种说法似乎微不足道，但事实证明，它是比较不同表示形式表达能力的重要工具）。

如这个简单的示例所示，某些分布族（在这种情况下，是由n个独立随机变量生成的分布）允许进行另一种参数化，该参数化比朴素的表示形式，也就是显式联合分布要紧凑得多。当然，在大多数实际应用中，随机变量并不是边际独立的。但是，这种方法的一般化将成为我们解决方案的基础。

### 3.1.2 条件参数化

让我们从一个说明基本直觉的简单示例开始。 考虑一下一家公司试图聘用应届大学毕业生所面临的问题。 该公司的目标是雇用聪明的员工，但无法直接测试智力。 但是，该公司可以访问学生的SAT分数，该分数仅供参考，不能完全说明问题。 因此，我们的概率空间是由两个随机变量Intelligence (I)和SAT (S)引起的。 为了简单起见，我们假设每个变量都采用两种值：$Val(I)= \{i_1，i_0\}$，分别代表智力高($i_1$)和智力低($i_0$)； 同样，$Val(S)= \{s_1，s_0\}$，它们分别也代表分数高和分数低。因此，在这种情况下，我们的联合分布有四个条目。 例如，一种可能的联合分布P为

| I     | S     | P(I, S) |
| ----- | ----- | ------- |
| $i^0$ | $s^0$ | 0.665   |
| $i^0$ | $s^1$ | 0.035   |
| $i^1$ | $s^0$ | 0.06    |
| $i^1$ | $s^1$ | 0.24    |

（表(3.2)）

但是，存在另一种更自然的方式来表示相同的联合分布。 使用条件概率的链式规则（请参阅公式(2.5)），我们可以得出

$P(I, S) = P(I)P(S\mid I)$

直观地，我们以更符合因果关系的方式表示这个过程。首先， 存在各种因素（遗传，成长……），（随机地）决定了学生的智力。 他在SAT上的表现（随机地）取决于他的智力。 我们注意到，我们构建的模型不需要遵循因果直觉，但通常会遵循因果直觉。 稍后我们将继续探讨这个问题。

从数学角度看，该方程式导致以下表示联合分布的替代方法。 我们将以$P(I)$和$P(S\mid I)$的形式指定联合分布，而不是指定各种联合项$P(I, S)$。 因此，比方说，我们可以使用以下两个表来表示方程式(3.2)的联合分布，一个表代表I上的先验分布，另一个代表给定I的S的条件概率分布(CPD):

| $i^0$ | $i^1$ |
| ----- | ----- |
| 0.7   | 0.3   |

| I     | $s^0$ | $s^1$ |
| ----- | ----- | ----- |
| $i^0$ | 0.95  | 0.05  |
| $i^1$ | 0.2   | 0.8   |

（表(3.3)） 

CPD $P(S \mid I)$表示学生在两种可能的情况下成功通过SAT的概率：学生的智力低和智力高。 CPD断言，智力低的学生极不可能获得高SAT分数($P(s^1 \mid i^0)= 0.05$)；另一方面，智力高的学生很可能（但不确定）会获得较高的SAT分数($P(s^1|i^1)= 0.8$)。

考虑如何对这种替代表示进行参数化具有指导意义。在这里，我们使用三个二项分布，一个用于$P(I)$，两个用于$P(S | i^0)$和$P(S \mid i^1)$。因此，我们可以使用三个独立的参数$\theta_{i^1}$，$\theta_{s^1|i^1}$和$\theta_{s^1|i^0}$来参数化此表示。因为我们将联合分布表示为四结果多项分布也需要三个参数。所以，尽管条件表示比关节的显式表示更自然，但条件并不更紧凑。但是，正如我们将很快看到的，条件参数化为我们更复杂的分布的紧凑表示提供了基础。

虽然我们仅在3.2.2节中正式定义贝叶斯网络，但了解如何将此示例中的情形用一个贝叶斯网络表示是很有启发性的。如图3.1a所示，贝叶斯网络分别用两个节点表示两个随机变量I和S，从I到S的边代表此模型中依赖关系的方向。

![](./pic/001.png)

### 3.1.3 朴素贝叶斯模型

现在，我们描述最简单的示例，其中将条件参数化与条件独立性假设相结合，以产生非常紧凑的高维概率分布表示。 重要的是，与前面的完全独立随机变量示例不同，此分布中的任何变量都不会（边际）独立。

#### 3.1.3.1 学生例子

详细说明我们的示例，我们现在假设该公司在某些课程中还可以访问该学生的分数G。 在这种情况下，我们的概率空间是三个相关随机变量I，S和G的联合分布。假设I和S像以前一样，并且G取三个值$g^1，g^2，g^3$，代表等级A ，B和C，则联合分布有十二个项。

在此示例中，哪怕是还没开始考虑分布P的具体数值之前，我们可以看到独立性无济于事：对于任何合理的P，都没有独立性。 显然，学生的智力与他的SAT分数和成绩都相关。 SAT分数和成绩也不是独立的：如果我们以学生在SAT上获得高分这一事实为条件，那么他在班上获得高分的机会也有可能增加。 从而，我们可以假设，对于我们的特定分布P，$P(g^1\mid s^1)>P(g^1\mid s^0)$

但是，在这种情况下我们的分布P满足条件独立性却是很合理的。 如果我们知道学生具有很高的智力，那么SAT的高分将不再向我们提供有关该学生在课堂上的表现的信息。 更正式地：

$P(g\mid i^1, s^1)=P(g\mid i^1)$

更一般地，我们可以假设

$P \models (S \perp G \mid I) \tag{3.4}$

请注意，只有当我们假设学生的智力是与成绩和SAT分数唯一可能相关的原因时，此独立性声明才成立。 换句话说，它假设基于其他因素（例如学生参加定时考试的能力）没有相关性。 从任何形式的意义上讲，这些假设也不是“现实的”，它们通常只是我们真实信念的近似（有关更多讨论，请参见专栏3.C）。

与边际独立的情况一样，条件独立允许我们提供联合分布的紧凑表示。 同样，紧凑表示法是基于非常自然的替代参数设置。 通过简单的概率推理（如公式(2.5)所示），

$P(I, S, G)=P(S, G\mid I)P(I)$

但是现在，方程式(3.4)的条件独立性假设意味着

$P(S, G\mid I)=P(S\mid I)P(G\mid I)$

因此，我们有

$P(I, S, G)=P(S\mid I)P(G\mid I)P(I) \tag{3.5}$

因此，我们将联合分布$P(I，S，G)$分解为三个条件概率分布(CPD)的乘积。 这种分解使我们立即找到所需的替代参数。 为了完全指定满足方程(3.4)的联合分布，我们需要以下三个CPD：$P(I)，P(S \mid I)和P(G \mid I)$。 前两个可能与公式(3.3)中的相同。 后者可能是

| I     | $g^1$ | $g^2$ | $g^3$ |
| ----- | ----- | ----- | ----- |
| $i^0$ | 0.2   | 0.34  | 0.46  |
| $i^1$ | 0.74  | 0.17  | 0.09  |

这三个CPD一起完全指定了联合分布（假设方程式（3.4）的条件独立）。 例如，

$P(i^1, s^1, g^2)$	$=P(i^1)P(s^1\mid i^1)P(g^2)$

​						$ = 0.3 \cdot 0.8 \cdot 0.17 = 0.0408$

再次，我们注意到，该概率模型将使用图3.1b中所示的贝叶斯网络表示。

在这种情况下，参数化比联合分布的参数化更紧凑。现在，我们有三个二项分布 —— $P(I), P(S\mid i^1)和P(S\mid i^0)$，以及两个三值多项分布 —— $P(G\mid i^1)$和$P(G\mid i^0)$。每个二项分布都需要1个独立参数，每个三值多项分布都需要2个独立参数，总共是7个独立参数。相比之下，我们的联合分布有12个条目，因此需要11个独立参数来指定这三个变量的任意联合分布。

重要的是要注意这种表示联合分布的方式的另一个优点：模块化。当我们添加新变量G时，联合分布完全改变了。如果我们使用联合分布的显式表示形式，我们将不得不写下十二个新数字。在分解式表示中，我们可以为变量I和S重用我们的局部概率模型，并仅指定G的概率模型 —— CPD $P(G\mid I)$。事实证明，此性质在对现实系统进行建模时具有不可估量的价值。

#### 3.1.3.2 一般化模型

这个例子是一个更通用的模型的实例，通常称为朴素贝叶斯(naive Bayes)模型（也被称为Idiot Bayes model ）。

![](./pic/002.png)

朴素的贝叶斯模型假设存在许多互斥的枚举类(class)，实例(instance)可被划分为其中一类。 因此，我们有一个类变量C，它采用某个集合$\{c_1, \cdots, ck\}$中的值。 在我们的示例中，类别变量是学生的智力I，并且有两类实例——智力高的学生和智力低的学生。

该模型还包括一些特征$X_1, \cdots, X_n$，这些特征的值通常已经被观察到了。 朴素的贝叶斯假设是，给定实例的类，这些特征在条件上是独立的。 换句话说，在每一类实例中，可以独立地确定不同的属性。 更正式地说，我们有

$P(C, X_1, \cdots, X_n)=P(C)\prod\limits_{i=1}^nP(X_i\mid C) \tag{3.7}$

（请参阅练习3.2）。因此，在此模型中，我们可以使用一个规模不大的因子集合来表示联合分布：先验分布P(C)，指定一个实例属于先验的不同类别的可能性，以及一组 CPD $P(X_j\mid C)$，对应了n个已知变量。 可以使用少数的参数对这些因子进行编码。 例如，如果所有变量都是二进制变量，则指定分布所需的独立参数的数量为2n +1（请参阅练习3.6）。 因此，参数的数量对变量的数量而言是线性变化的，与联合分布的显式表示的指数变化形式不同。

**专栏3.A — 概念：朴素贝叶斯模型。** 尽管具有朴素的贝叶斯模型，但因为朴素的贝叶斯模型简单且所需参数数量少，所以在实践中经常被使用。 该模型通常用于分类 —— 根据给定实例的证据变量的值确定该实例最可能属于的类。 我们可能还想计算我们对该决策的信心，即我们的模型在多大程度上偏爱一个类别$c^1$而不是另一个类别$c^2$。 可以通过以下比率来解决这两个查询：

$\frac{P(C=c^1\mid x_1, \cdots, x_n)}{P(C=c^2\mid x_1, \cdots, x_n)} = \frac{P(C=c^1)}{P(C=c^1)}\prod\limits_{i=1}^n\frac{P(x_i\mid C=c^1)}{P(x_i\mid C=c^2)} \tag{3.8}$

（见练习3.2）。 该公式非常自然，因为它将$c^1$与$c^2$的后验概率计算成它们的先验概率比（式子中的第一项），乘以一组在观察到 $x_i$ 时能够度量两个类别相对支持度的项$\frac{P(x_i\mid C=c^1)}{P(x_i\mid C=c^2)}$。

此模型用于医学诊断的早期，其中类别变量的不同值代表患者可能患有的不同疾病。 证据变量表示不同的症状，检查结果等。 请注意，该模型做出了一些通常不成立的强假设，特别是：患者最多只能患有一种疾病，并且在给定患者所患疾病的前提下，是否存在不同的症状或检查结果都是独立的。 该模型可用于医学诊断，因为参数可解释并且数量少，因此很容易从专家那里得出。 例如，很自然的，问专家医生肺炎患者发高烧的可能性是多少。 的确，有几种早期的医学诊断系统都基于该技术，并且已经证明某些系统提供的诊断要比专家医生提供的诊断更好。

但是，后来的经验表明，该模型的强假设降低了其诊断准确性。尤其是，该模型倾向于通过“过量计数(*overcounting* )”来高估某些证据的影响。例如，高血压和肥胖症都是心脏病的有力指标。但是，因为这两种症状本身是高度相关的，所以方程式(3.8)对每个症状都包含一个相乘项，这就重复计算了它们提供的有关疾病的证据。确实，一些研究表明，朴素贝叶斯模型的诊断性能会随着特征数量的增加而降低。这种降低通常可以追溯到违反强条件独立性假设的行为。这种现象导致在此应用中使用更复杂的贝叶斯网络（具有更现实的独立性假设，请参见专栏3.D）。

尽管如此，朴素的贝叶斯模型仍然可以在各种应用中使用，特别是在从具有大量特征和相对较少实例的领域数据中学习的模型的背景下，例如以文档中的单词为特征（见专栏17.E）。

## 3.2 贝叶斯网络

贝叶斯网络通过利用分布的条件独立性来建立与朴素贝叶斯模型相同的直觉，从而实现紧凑自然的表示。 但是，它们不限于表示那些满足朴素贝叶斯模型中隐含的强独立性假设的分布。 它们使我们可以灵活地将分布的表示方式调整为在当前情形下合理的独立性。

贝叶斯网络表示法的核心是有向无环图(DAG) G，它的节点是我们的域中的随机变量，它的边直观地对应于一个节点对另一节点的影响。 可以通过两种截然不同的方式审视图G：

- 作为一种数据结构，提供了一种以分解的方式来紧凑地表示联合分布的骨架
- 作为关于分布的一组条件独立性假设的紧凑表示

正如我们将看到的，毫无疑问，这两种观点是等效的。

### 3.2.1 学生例子回顾

我们从一个简单的玩具示例开始我们的讨论，该示例将在本书的大部分内容中以各种版本伴随我们。

#### 3.2.1.1 模型

![](./pic/003.png)

考虑我们之前的学生，但现在考虑一个稍微复杂的场景。在这种情况下，学生的成绩不仅取决于他的智力，还取决于课程的难度，课程难度由一个随机变量D表示，其域为$Val(D) = \{easy，hard\}$。到了期末的时候，我们的学生向他的教授索要推荐信。这位教授心不在焉，并且从不记得她学生的名字。她只能看他的成绩，仅根据这些信息就可以为他写信。她的信的质量是一个随机变量L，其域为$Val(L) = \{strong，weak\}$。信件的实际质量随机取决于成绩（也可能取决于教授的压力和当天早上喝咖啡的质量）。

因此，我们在整个域中有五个随机变量：学生的智力(I)，课程难度(D)，成绩(G)，学生的SAT分数(S)和推荐信的质量(L) 。除G以外的所有变量均为二进制值，而G为三进制值。因此，联合分布有48个项。

正如我们在图3.1的简单图示中看到的那样，贝叶斯网络是用有向图表示的，该图的节点表示随机变量，其边表示一个变量对另一个变量的直接影响。我们可以将图视为自然执行的生成采样过程的编码，在这个过程中每个变量的值都是用一个分布自动选择出来的。换句话说，每个变量都是其父节点的随机函数。

基于这种直觉，想必本例中能最自然地表示概率分布的网络结构就是图3.3中的那个。 图的边反映了我们对世界运作方式的直觉。 课程难度和学生的智力是在模型中的任何变量之前独立确定的。 学生的成绩取决于这两个因素。 学生的SAT分数仅取决于他的智力。 教授推荐信的质量（假设）仅取决于班级学生的成绩。 直观地讲，模型中的每个变量仅直接取决于网络中的父变量。 我们稍后将这种直觉形式化。

贝叶斯网络表示法的第二个组成部分是一组局部概率模型，这些模型表示了每个变量对其父亲的依赖的自然特性。其中模型P(I)代表了聪明学生与不聪明学生的人口分布，P(D)表示难易课程的分布。学生成绩的分布是条件分布$P(G \mid I,D)$。它指定了学生成绩的分布情况，这取决于学生的智力和课程难度。具体来说，对于值i, d的每种赋值，我们将具有不同的分布。例如，我们可能认为，简单课程上的聪明学生获得A的可能性为90％，B上学生的可能性为8％，而C上为2％。相反，困难课程上的聪明学生获得A的可能性仅为50％。推广来说，模型中的每个变量X都与条件概率分布（CPD）相关，在给定模型中该变量所有父亲的联合赋值时，条件概率分布指定了X值的分布。对于没有父节点的节点，CPD取决于空变量集。故CPD表现为边际分布，例如P(D)或P(I)。图3.4显示了在本问题的域上的一种可能的CPD。网络结构与其CPD共同构成了贝叶斯网络B；我们使用 $B^{student}$ 作为学生示例中贝叶斯网络的引用。

![](./pic/004.png)

我们如何使用此数据结构指定联合分布？ 考虑该空间中的某个特定状态，例如$i^1, d^0, g^2, s^1, l^0$。 从直觉上讲，可以从构成该事件的基本事件的概率中计算出该事件的概率：学生是聪明的概率； 课程容易的概率； 聪明的学生在简单课程上获得B的概率； 聪明的学生在SAT上获得高分的概率； 以及在课程中获得B的学生得到弱推荐信的可能性。 此状态的总概率为：

$P(i^1, d^0, g^2, s^1, L^0)$	$ = P(i^1)P(d^0)P(g^2\mid i^1, d^0)p(s^1\mid i^1)P(l^0\mid g^2)$

​									$ = 0.3 \cdot 0.6 \cdot 0.08 \cdot 0.8 \cdot 0.4 = 0.004608$

显然，我们可以对联合概率空间中的任何状态使用相同的过程。 一般地，我们有

$P(I, D, G, S, L) = P(I)P(D)P(G\mid I, D)P(S\mid I)P(L\mid G) \tag{3.9}$

该方程式是我们贝叶斯网络链式法则的第一个示例，我们将会在第3.2.3.2节中在一般情形下定义该法则。

#### 3.2.1.2 推理模式

联合分布$P_B$指定（尽管是隐式的）给定任意观测值e时任意事件y发生的概率$P_B(Y = y \mid E = e)$，如第2.1.3.3节所述：我们将联合分布以事件E = e为条件消除与我们的观测值e不一致的联合分布中的项，并重新归一化结果项的总和，使之为1；我们通过将后验分布中与y一致的所有项的概率相加来计算事件y的概率。为了说明这一过程，让我们考虑一下我们的$B^{student}$网络，看看随着证据的获得，各种事件的概率如何变化。

考虑一个特定的学生，乔治，我们想使用我们的模型对这个学生进行推理。我们可能会问乔治在Econ101中得到他的教授写强推荐信的可能性($l^1$)。若对乔治或Econ101一无所知，则这个概率约为50.2％。更准确地说，令$P_{B^{student}}$为先前的BN定义的联合分布；那么我们$P_{B^{student}}(l^1)\approx 0.502$。现在我们发现乔治不是那么聪明($i^0$)；他从Econ101教授那里收到一封强推荐信的可能性下降到38.9％左右；也就是说，$P_{B^{student}}(l^1\mid i^0)\approx 0.389$。现在，我们进一步发现Econ101是一个简单的课($d^0$)。乔治从教授那里收到一封强推荐信的概率现在为$P_{B^{student}}(l^1\mid i^0, d^0)\approx 0.513$。诸如此类的查询是我们预测各种因素（例如乔治的智力）对“下游”的影响的因果推理(causal reasoning)或预测(prediction)的例子。

现在，考虑Acme Consulting的招聘人员，尝试根据我们的模型来决定是否雇用乔治。先验地，招聘人员认为，乔治有30％的概率是聪明的。他获得了乔治在特定课程Econ101中的成绩记录，并且看到乔治在该课程中中获得了C($g^3$)。他认为乔治具有高智商的可能性大大降低，下降到7.9％左右。也就是说，$P_{B^{student}}(i^1\mid g^3)\approx 0.079$。我们注意到，该课程是困难课程的可能性也从40％上升到62.9％。

现在，假设招聘者（对乔治来说十分幸运）丢失了乔治的成绩单，并且只收到了乔治的教授在Econ101中的弱推荐信（不足为奇）。 乔治具有高智能的可能性仍然下降，但仅下降到14％：$P_{B^{student}}(i^1 \mid l^0)\approx 0.14$。 请注意，如果招聘人员同时具有分数和推荐信，则我们拥有与只有分数的概率相同：$P_{B^{student}}(i^1\mid g^3, i^0)\approx 0.079$;；我们将在以后再次讨论这个问题。 诸如此类的查询是我们从结果(effects)到原因(causes)的推理，是证据推理(evidential reasoning)或解释(explanation)的例子。

最后，乔治将他的SAT分数提交给了招聘者，令人惊讶的是，他的SAT分数很高。 乔治具有高智商的概率从7.9％急剧上升至57.8％：$P_{B^{student}}(i^1\mid g^3, s^1)\approx 0.578$。 直观上，高SAT成绩胜过较差成绩的原因是，智力低的学生极不可能在SAT上获得高分，而智力高的学生仍能获得C。 但是，聪明的学生更有可能在难度高的课程上获得C。 确实，我们看到Econ101是高难度课程的可能性从我们之前看到的62.9％上升到大约76％。

最后一种推理模式特别有趣。 有关SAT的信息为我们提供了有关学生智力的信息，再结合该学生的学业成绩，可以告诉我们有关该课程难度的信息。 实际上，我们有一个关于成绩变量的起因(causal factor)——学生智力——为我们提供了另一个起因——课程难度——的信息。

让我们单纯从形式上检查此模式。就像我们说过的，$P_{B^{student}}(i^1\mid g^3)\approx 0.079$。另一方面，如果现在我们发现Econ101是一个高难度课程，则我们的$P_{B^{student}}(i^1\mid g^3, d^1)\approx 0.11$。实际上，我们至少对Econ101中乔治的分数做了部分解释。举一个更加惊人的例子，如果乔治在Econ 101中获得B，则我们的$P_{B^{student}}(i^1\mid g^2)\approx 0.175$。另一方面，如果Econ101是一个高难度课程，我们得到$P_{B^{student}}(i^1\mid g^2, d^1)\approx 0.34$。实际上，我们已经通过课程的困难辩解(explaining away)了成绩差的原因。**辩解(explaining away)是称为因果间推理(intercausal reasoning)的一般推理模式的一个实例，其中相同结果的不同原因可以相互作用。这种推理是人类推理中非常普遍的模式。**例如，当我们发烧，嗓子疼，担心单核细胞增多症时，得知自己患有流感，我们就大为放心。显然，患有流感并不能阻止我们患有单核细胞增多症。但是，得了流感后，我们可以对症状进行另一种解释，从而大大降低了单核细胞增多症的可能性。

提供给定证据的另一种解释的直觉可以非常精确。如练习3.3所示，如果流感确是引起症状的原因，则单核细胞增多症的可能性将降至其先前的可能性（观察到任何症状之前的可能性）。另一方面，如果流感可能在没有引起这些症状的情况下发生，则单核细胞增多症的可能性会下降，但仍比其基本水平高一些。但是，解释并不是因果间推理的唯一形式。影响可以向任何方向发展。例如，考虑一种情况，即有人在死亡后被发现了而且可能是被谋杀的。犯罪嫌疑人具有动机和机会的概率都在上升。如果我们现在发现嫌疑人有动机，那么他有机会的可能性就会上升（请参阅练习3.4）。

需要强调的是，尽管我们的解释使用了诸如原因和证据之类的直观概念，但是我们进行的概率计算并没有什么神秘之处。可以简单地通过生成联合分布（如公式(3.9)中定义）并直接从中计算各种事件的概率来复现它们。

### 3.2.2 贝叶斯网络中的基本独立性

#### 3.2.2.1 学生例子中的独立性

在学生示例中，我们使用边即直接依赖的直觉。 例如，我们做出了诸如“教授的推荐信仅取决于班级中学生的成绩”直观陈述；该语句是基于以下事实而被编码进图中的：除G外没有其它指向L节点的边。这种直觉是“一个节点仅由其父节点直接决定”，这是贝叶斯网络语义的核心。

我们使用条件独立性语句为该断言提供形式语义。 例如，先前的断言可以正式地陈述为假设L在给定其父节点G的情况下条件独立于网络中的所有其他节点：

$(L\perp I, D, S\mid G) \tag{3.10}$

换句话说，一旦我们知道了学生的成绩，我们对他的推荐信质量的信念就不会受到任何其他变量信息的影响。 类似地，为了使我们的直觉——学生的SAT分数仅取决于他的智力——正式化，我们可以说S在有父节点I的情况下条件独立于网络中的所有其他节点：

$(S\perp D, G, L\mid I) \tag{3.11}$

现在，让我们考虑G节点。 若盲目地遵循上述模式，我们可能会试图断言G给定所有父节点的情况下条件独立于网络中的所有其他变量。 但是，这种假设在直觉上和我们之前使用的特定分布的例子中都是错误的。 例如，假定我们以$i^1，d^1$为条件； 也就是说，我们有一个聪明的学生上难上课。 在这种情况下，G是否独立于L？ 显然，答案是否定的：如果我们观察到$l^1$（学生收到一封强推荐信），那么我们的 $g^1$（学生在课程中获得A）的概率应该上升； 也就是说，我们期望

$P(g^1\mid i^1, d^1, l^1) > P(g^1\mid i^1, d^1)$

的确，如果我们检查分布，则后者的概率为0.5（在CPD中指定），而前者的概率高得多，为0.712。
因此，我们看到，在给定父节点的情况下，我们不希望节点条件独立于所有其他节点。 特别是，即使给定父节点，它仍然可以依赖于其后代节点。 可以依赖其他节点吗？ 例如，在给定I和D的情况下，我们是否期望G取决于S？ 凭直觉，答案是否定的。 一旦我们知道该学生的智力高，他的SAT分数就不会给我们提供与预测其成绩相关的其他信息。 因此，我们希望该属性具有：

$(G\perp S\mid I, D) \tag{3.12}$

现在只剩下变量I和D需要我们考虑，它们在图中没有父节点。所以我们在给定节点的父节点然后寻找其独立性的过程中实际上是在寻找边际独立性。 如前面的讨论所示，在我们的分布$P_{B^{student}}$中，I并不独立于其后代G，L或S。I的唯一非后代节点是D。实际上，我们隐式地假设Intelligence和Difficulty是独立的。 因此，我们期望：

$(I\perp D) \tag{3.13}$

鉴于我们之前的例子，这种分析似乎有些令人感到惊讶，在之前的例子中，得知的课程难易程度将极大地改变我们对学生智力的看法。 但是其实，在那种情况下，我们是在根据有关学生成绩的信息进行推理。 换句话说，我们在给定G的情况下证明了I和D的依赖性。这一现象非常重要，我们之后将继续讨论。

对于变量D而言，I和S均为非后代。 回想一下，如果$(I\perp D)$ ，则$(D\perp I)$。 变量S增加了我们对学生智力的信念，但是知道学生聪明（或不聪明）并不会影响我们对课程难度的信念。 因此，我们有

$(D\perp I, S) \tag{3.14}$

我们可以看到一种模式正在浮出水面。 直觉告诉我们，变量的父节点将变量受到的由因果关系激发的概率作用给“屏蔽”掉了。 换句话说，对一个节点而言，一旦我观察到其父节点的值，任何与其祖先节点（无论是父亲还是其它祖先）相关（无论是直接还是间接）的信息都已无法影响我对这个节点的信念。 即便如此，有关该节点后代的信息仍然可以通过证据推理过程来改变我对该节点的信念。

#### 3.2.2.2 贝叶斯网络的语义

现在，我们准备提供贝叶斯网络结构语义的正式定义。 我们希望正式定义与我们的示例中开发的直觉相匹配。

（**定义3.1** 贝叶斯网络结构，局部独立性）贝叶斯网络结构G是有向无环图，其节点表示随机变量$X_1, \cdots X_n$。 令$Pa^G{X_i}$表示G中$X_i$的所有父亲，$NonDescendants_{X_i}$表示图中不是$X_i$的后代的变量。 于是，G就编码了以下被称作局部独立性的记为$I_l(G)$的条件独立假设集合：

对每个变量$X_i: (X_i \perp NonDescendants_{X_i} \mid Pa^G_{X_i})$

换句话说，局部独立性指出，对每个节点$X_i$ ，在给定其父节点后，它条件独立于它的非后代节点。

回到学生网络$G_{student}$，局部的马尔可夫独立性正是由我们的直觉所决定的，并在公式(3.10) - 公式(3.14)中指定。

![](./pic/005.png)

**专栏 3.B —— 案例研究：遗传学实例。**  贝叶斯网络模型最早的用途之一（早于定义一般框架）就在遗传谱系领域。 在这种情况下，局部独立性特别直观。 在此应用中，我们希望对从父母到孩子的某种属性（例如血型）的传播进行建模。 一个人的血型是一个可观察到的量，取决于他的基因组成。 这种特性称为表现型(phenotypes)。 一个人的基因组成被称为基因型(genotype )。

为了正确地模拟这种情况，我们需要介绍一些遗传学背景。 人类遗传物质包括22对常染色体(autosomal chromosomes)和一对性染色体(sex chromosomes)。 每个染色体都包含一组遗传物质，这些遗传物质（除其他事项外）由决定一个人特性的基因组成。 一个我们关注的染色体区域被称为基因座(locus)。 而基因座上的DNA序列可能有许多不同的变化，各种变化形式称为等位基因(allele)。

为了具体起见，我们关注常染色体染色体对。 在每个常染色体对中，一个染色体是从父亲遗传的父染色体，另一个是从母亲遗传的母染色体。 对于常染色体对中的基因，一个人有该基因的两个副本，每个染色体各一个副本。 因此，该基因的一个等位基因是从这个人的母亲继承的，而另一个是从这个人的父亲继承的。 例如，包含编码人血型的基因区域就是一个基因座。 这个基因有3个变体或等位基因：A，B和O。因此，一个人的基因型由有序对表示，例如⟨A，B⟩。 对于该对中的每个条目有3个选择，则有9种可能的基因型。 血型表现型是基因的两个拷贝的函数。 例如，如果此人有一个A等位基因和一个O等位基因，则她观察到的血型为“ A”。如果她有两个O等位基因，则观察到的血型为“ O”。

为了表示这个域，我们每个人都有两个变量：一个代表这个人的基因型，另一个代表她的表现型。 我们用名字G(p)代表人p的基因型，用名字B(p)代表她的血型。

在此示例中，独立性假设直接来自生物学。 由于血型是基因型的函数，所以一旦我们知道一个人的基因型，有关家庭其他成员的其他证据就不会提供有关血型的新信息。 同样，遗传继承的过程也意味着独立性假设。 一旦我们知道了父母双方的基因型，我们就会知道他们每个人将什么基因型传给了后代。 因此，学习有关祖先（或后裔）的新信息不会提供有关后代基因型的新信息。 这些恰好是得到的网络结构中的局部独立性，如图3.B.1中的简单族谱所示。 这里的直觉很清楚； 例如，巴特（Bart）的血型与他的姨妈塞尔玛（Selma）的血型相关，但是一旦我们知道荷马（Homer）和玛格（Marge）的基因型，两者就成为独立的。

要完全定义概率模型，我们需要指定CPD。 此模型中共有三种CPD：

- 外显率模型$P(B(c)\mid G(c))$描述了给定人基因型的特定表型（如不同血型）不同变体的概率。 对于血型，此CPD是确定的函数，但在其他情况下，依赖性可能更复杂。
- 传递模型$P(G(c)\mid  G(p), G(m))$，其中c是一个人，p和m是她的父亲和母亲。 每个父母都有可能将其两个等位基因中的一个传播给孩子。
- 基因型先验$P(G(c))$，当人c在谱系中没有父母时使用。 这些是种群中一般的基因型频率。

由于多种原因，我们对血型的讨论得以简化。首先，某些表现型（例如迟发性疾病）不是该基因型的确定性功能。但具有特定基因型的个体比具有其他基因型的个体更可能患有该疾病。其次，一个人的遗传构成由许多基因定义。一些表现型可能依赖于多个基因。在其他情况下，我们可能会对多种表现型感兴趣，这（自然地）意味着依赖于几个基因。最后，正如我们现在所讨论的，不同基因的遗传模式并非彼此独立。

回想一下，每个人的常染色体都是从她的父亲或母亲处继承的。但是，每个亲本的每个常染色体也都有两个副本。在每个父母中，这两个副本重组产生染色体，并传递给孩子。因此，巴特（Bart）所继承的母亲染色体是他的母亲玛格从母亲杰基（Jackie）和父亲克兰西（Clancy）继承的染色体的组合。重组过程是随机的，但是在一代人中只有少数重组事件发生在染色体内。因此，如果巴特从其母亲从其母亲杰基继承的染色体中继承了某个基因座的等位基因，则他也很有可能从附近基因座继承杰基的拷贝。因此，要为多基因座遗传构建合适的模型，我们必须考虑在相邻基因座对之间发生重组的可能性。

我们可以通过引入选择器变量来促进这种建模，这些选择器变量捕获沿染色体的继承模式。特别是，对于每个基因座l和每个孩子c，我们都有一个变量$S(l, c, m)$，如果c的母亲染色体中的基因座l是从c的祖母那里继承的，则取值为1；如果该基因座是c，则取值为2。继承自c的外祖父。对于c的父系染色体，我们有一个类似的选择器变量$S(l, c, p)$。现在，我们可以通过关联相邻基因座l，l'的变量$S(l, c, m)$和$S(l', c, m)$来建模由低重组频率引起的相关。

这种类型的模型已广泛用于许多应用程序。在遗传咨询和预测中，一个人采取具有已知基因座的表型，并为家谱中的某些个体获取一组观察到的表型和基因型数据，以推断谱系中另一个人（例如，计划中的孩子）的基因型和表型。遗传数据可以包括对相关疾病位点（对于某些个体）的直接测量或与疾病位点相关的附近基因座的测量。

在连锁分析中，这项任务比较艰巨：使用一定数量的谱系从谱系数据中识别疾病基因的位置，其中大部分个体表现出疾病表型。在这里，可用数据包括谱系中许多个体的表型信息，以及染色体中已知位置的基因座的基因型信息。使用遗传模型，研究人员可以根据疾病基因相对于已知基因座的位置的不同假设，评估这些观察结果的可能性。通过反复计算网络中不同假设的概率，研究人员可以查明与疾病“相关”的区域。然后，可以将该较小的区域用作对该区域中的基因进行更详细检查的起点。这个过程至关重要，因为它可以使研究人员专注于较小的区域（例如，基因组的1 / 10,000）。

正如我们将在后面的章节中看到的那样，使用稀疏贝叶斯网络描述遗传过程的能力为我们提供了使用复杂推理算法的能力，这些推理算法使我们能够推断出大的谱系和多个基因座。它还允许我们使用算法进行模型学习，以更深入地了解遗传遗传过程，例如不同区域的重组率或不同疾病的渗透率。

### 3.2.3 图与分布

贝叶斯网络图的形式语义是一组独立性断言。 另一方面，我们的学生BN是带有CPD注释的图，该图通过贝叶斯网络的链式法则定义了联合分布。 在本节中，我们显示这两个定义实际上是等效的。 分布P满足图G上的局部独立性，当且仅当P可用图G上的那组CPD表示。我们从形式化基本概念开始。

### 3.2.3.1 I-Maps

我们首先定义与分布P上的独立性集合。

（**定义3.2** P中的独立性）令P为X的分布。我们将I(P)定义为在P中满足 $(X\perp Y\mid Z)$ 形式的独立性断言的集合。

现在，我们可以将“ P满足G上的局部独立性”的陈述改写为$I_l(G) \subseteq I(P)$。 在这种情况下，我们说G是P的I-map(independency map)。此外，推广此概念是很有用的，因为在本书中将使用其不同的变体。

（**定义3.3** I-map）令$K$为满足独立性$I(K)$的任意图对象。 我们说，如果$I(K) \subseteq I$，则$K$是独立性集合$I$的I-map。

现在我们可以说，若G对I(P)而言是一个I-map，则G对P而言也是一个I-map。

从包含的方向可以看出，要使G成为P的I-map，就要使G不会在P的独立性上误导我们：G断言的任何独立性也必须存在于P中。相反，P可能具有其他独立性，这些独立性并没有体现在G中。

让我们通过一个非常简单的示例来说明I-map的概念。

（例子3.1）考虑两个独立随机变量X和Y的联合概率空间。 在这两个节点上有三个可能的图形：$G_\empty$，它是一个不联通的的点对X  Y； $G_{X\rightarrow Y}$，其包含边$X\rightarrow Y$； 和$G_{Y\rightarrow X}$，其包含边$Y\rightarrow X$。 图$G_\empty$对假设$(X\perp Y)$进行编码。 后两个没有编码独立性假设。

考虑以下两个分布：

| X     | Y     | P(X, Y) |
| ----- | ----- | ------- |
| $x^0$ | $y^0$ | 0.08    |
| $x^0$ | $y^1$ | 0.32    |
| $x^1$ | $y^0$ | 0.12    |
| $x^1$ | $y^1$ | 0.48    |

| X     | Y     | P(X, Y) |
| ----- | ----- | ------- |
| $x^0$ | $y^0$ | 0.4     |
| $x^0$ | $y^1$ | 0.3     |
| $x^1$ | $y^0$ | 0.2     |
| $x^1$ | $y^1$ | 0.1     |

在第一个分布中，X和Y在P中独立； 例如，$P(x^1)= 0.48 + 0.12 = 0.6, P(y^1)= 0.8, P(x^1, y^1)= 0.48 = 0.6 \cdot 0.8$。 因此，$(X \perp Y) \in I(P)$，我们有$G_{\empty}$是P的I-map。实际上，所有三个图都是P的I-map：$I_l(G_{X\rightarrow Y})$是空的，所以平凡的P满足了其中的所有独立性（$G_{Y\rightarrow X}$ 也是类似的）。 在第二个分布中，$(X \perp Y) \notin I(P)$，因此$G_{\empty}$不是P的I-map。另外两个图也不是P的I-map。

#### 3.2.3.2 I-Map to Factorization

BN结构G编码一组条件独立性假设；使得G是I-map的任意分布都必须满足这些假设。 该属性是允许使用在第3.2.1节的“学生”示例中看到的紧凑分解式表示的关键。 基本原理与我们在3.1.3节中朴素贝叶斯分解中使用的原理相同。

考虑使得我们的学生BN $G_{student}$ 是I-map的任意分布P。 我们将分解联合分布，并证明它可以被分解为局部概率模型，如3.2.1节所述。 考虑联合分布$P(I，D，G，L，S)$  ；从概率的链式法则（公式(2.5)），我们可以通过以下方式分解此联合分布：

$P(I, D, G, L, S)=P(I)P(D\mid I)P(G\mid I, D)P(L\mid I, D, G)P(S\mid I, D, G, L) \tag{3.15}$

此转换不依赖任何假设。它适用于任何联合分布P。但是，它也不是很有用，因为右侧分解的条件概率既非自然也不紧凑。例如，最后一个因子要求，对于变量 $i, d, g, l$ 的每种赋值，需要指定24个条件概率：$P(s^1\mid i, d, g, l)$。

但是，这种形式允许我们应用由BN产生的条件独立性假设。让我们假设$G_{student}$是我们分布P的I-map。特别是，根据式子(3.13)，我们有$(D \perp I) \in I(P)$。由此，我们可以得出结论：$P(D\mid I) = P(D)$，这使我们可以简化右侧的第二个因子。同样，我们由式子(3.10)可知$(L \perp I, D\mid G) \in I(P)$。因此，$P(L \mid I, D, G) = P(L\mid G)$，允许我们简化第三项。以类似的方式使用式子(3.11)，我们得到

$P(I, D, G, L, S)=P(I)P(D)P(G\mid I, D)P(L\mid G)P(S\mid I) \tag{3.16}$

这种分解正是我们在3.2.1节中使用的分解。

这个结果告诉我们，联合分布中的任何项都可以用因子的乘积来计算。 每个因子代表变量在网络中给定其父节点的条件概率。 此因式分解适用于可以以 $G_{student}$ 为I-map的任意分布P。

现在，我们更正式地陈述和证明这一基本结果。

（**定义3.4** factorization）令G为变量$X_1, \cdots, X_n$ 上的BN图。 我们说，如果相同空间上的分布P可以表示为乘积

$P(X_1, \cdots, X_n) = \prod\limits_{i=1}^nP(X_i\mid Pa_{X_i}^G) \tag{3.17}$

则分布P能在相同空间上根据G进行分解。该方程称为贝叶斯网络的链式法则。 个体因子$P(X_i\mid Pa^G_{X_i})$被称为条件概率分布(CPD)或局部概率模型。

（**定义3.5** 贝叶斯网络）贝叶斯网络是一个二元组B = (G，P)，其中P按照G进行了分解，并且被指定成为一组对应于G的节点的CPD。 分布P通常标记为$P_B$。

现在，我们可以证明在 $G_{student}$ 上观察到的现象在更一般的情况下也是能成立的。

（**定理3.1**）令G为一组随机变量X的BN结构，令P为同一空间上的联合分布。 如果G是P的I-map，则P能根据G分解。

证明：在不失一般性的前提下，假设$X_1, \dots, Xn$是X中变量相对于G的拓扑序（请参阅定义2.19）。 在我们的示例中，我们首先对概率使用链式法则：

$P(X_1,\dots, X_n) = \prod\limits_{i=1}^nP(X_i\mid X_1, \dots, X_{i-1})$

现在，考虑其中一个因子$P(X_i\mid X_1, \dots, X_{i-1}）$。 因为G是P的I-map，所以我们有$(X_i\perp NonDescendants_{X_i}\mid Pa^G_{X_i}) \in I(P)$。 根据假设， $X_i$的所有父节点都在集合 $X_1, \dots, X_{i-1}$ 中。 此外，$X_i$的任何后代都不可能在这个集合中。 因此，

$\{X_1, \dots, X_{i-1}\} = Pa_{X_i}\cup Z$

其中$Z \sube NonDescendants_{X_i}$。根据$X_i$ 的局部独立性和decomposition property(公式2.8) ，就能得到$(X_i\perp Z\mid Pa_{X_i})$ 。因此我们有

$P(X_i\mid X_1, \dots, X_{i-1})=P(X_i|Pa_{X_i})$

将这种变换应用于链式规则分解中的所有因素，结果如下。

因此，BN结构G隐含的条件独立性假设使我们能够将G为I-map的分布P分解为小的CPD。 注意，证明是有建设性的，在给定分布P和图G的情况下，提供了一种精确的算法来构造因式分解。

所得的因式分解表示可以实质上更紧凑，尤其是对于稀疏结构。





## 3.3 图的独立性

## 3.4 从分布到图



